"""
ê´‘ê³  ë¶„ì„ API ë¼ìš°íŠ¸
- 17ê°œ ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- íŒŒì¼ ì—…ë¡œë“œ ë° ë°ì´í„° ì²˜ë¦¬
"""

import os
import pandas as pd
import numpy as np
import logging
from flask import (
    Blueprint, render_template, request, jsonify,
    session, redirect, url_for, send_file, send_from_directory, current_app, g
)
from werkzeug.utils import secure_filename
import flask

from app.services.ad_analyzer import AdAnalyzer
from app.services.ai_insights import AIInsights
from app.utils.db_utils import execute_query, execute_insert, execute_update, DatabaseError
from app.utils.helpers import (
    allowed_file, clean_filename, get_unique_filename,
    create_error_response, create_success_response,
    ensure_directory_exists
)


from itsdangerous import URLSafeTimedSerializer

logger = logging.getLogger(__name__)

# ========================================
# ì†Œì…œ ë¯¸ë””ì–´ ë´‡ ê°ì§€
# ========================================
BOT_USER_AGENTS = [
    # ê¸€ë¡œë²Œ ì†Œì…œ/ë©”ì‹ ì €
    'facebookexternalhit',  # Facebook
    'Facebot',              # Facebook
    'Twitterbot',           # Twitter/X
    'LinkedInBot',          # LinkedIn
    'Slackbot',             # Slack
    'TelegramBot',          # Telegram
    'WhatsApp',             # WhatsApp
    'Discordbot',           # Discord
    'Pinterest',            # Pinterest

    # ê²€ìƒ‰ì—”ì§„
    'Googlebot',            # Google
    'bingbot',              # Bing

    # ë„¤ì´ë²„ ê´€ë ¨
    'Yeti',                 # Naver ê²€ìƒ‰
    'naver.me',             # Naver ê³µí†µ ì‹ë³„ì
    'NaverBot',             # Naver ë´‡
    'WorksOgCrawler',       # Naver Works OG í¬ë¡¤ëŸ¬
    'naverbookmarkcrawler', # Naver ë¶ë§ˆí¬
    'scrapbook-scraper',    # ìŠ¤í¬ë©ë¶

    # ì¹´ì¹´ì˜¤/ë‹¤ìŒ ê´€ë ¨
    'kakaotalk-scrap',      # KakaoTalk
    'Daumoa',               # Daum ê²€ìƒ‰
]

def is_social_bot(user_agent_string):
    """ì†Œì…œ ë¯¸ë””ì–´ ë´‡ì¸ì§€ í™•ì¸ (OG íƒœê·¸ í¬ë¡¤ëŸ¬)"""
    if not user_agent_string:
        return False
    ua_lower = user_agent_string.lower()
    return any(bot.lower() in ua_lower for bot in BOT_USER_AGENTS)

# ========================================
# ì œì™¸ í‚¤ì›Œë“œ íŒì • ìƒìˆ˜
# ========================================
EXCLUDE_MIN_SPEND = 5000      # ìµœì†Œ ê´‘ê³ ë¹„ (ì›)
EXCLUDE_MIN_CLICKS = 10       # ìµœì†Œ í´ë¦­ìˆ˜
EXCLUDE_CPC_CRITICAL = 500    # CPC ì‹¬ê° ê¸°ì¤€ (ì›)
EXCLUDE_CPC_VERY_HIGH = 800   # CPC ë§¤ìš° ë†’ìŒ ê¸°ì¤€ (ì›)
EXCLUDE_CLICKS_CRITICAL = 30  # ì „í™˜ì—†ìŒ ì¦‰ì‹œì œì™¸ í´ë¦­ìˆ˜
EXCLUDE_CLICKS_HIGH = 15      # ì „í™˜ì—†ìŒ ì¡°ì†íˆì œì™¸ í´ë¦­ìˆ˜

# ========================================
# ì„ì‹œ ì¸ì¦ í•¨ìˆ˜ (TODO: ì¶”í›„ ì¬ì„¤ê³„)
# ========================================
def get_current_user():
    user = {
        'userId': session.get('userId', 'test'),
        'userNicknm': session.get('userNicknm', 'testNicknm')
    }
    return user
    
def get_current_user_id():
    return session.get('userId', 'test')

# Blueprint ìƒì„±
ad_bp = Blueprint('ad_analysis', __name__)

# ========================================
# ì»¬ëŸ¼ ë§¤í•‘ ì •ì˜ (í•œê¸€ â†’ ì˜ë¬¸)
# ========================================
COLUMN_MAPPING = {
    'ë‚ ì§œ': 'date',
    'ìº í˜ì¸ëª…': 'campaign_name',
    'ê´‘ê³ ìœ í˜•': 'ad_type',
    'ì§€ì¶œì•¡': 'spend',
    'ë…¸ì¶œìˆ˜': 'impressions',
    'í´ë¦­ìˆ˜': 'clicks',
    'ì „í™˜ìˆ˜': 'conversions',
    'ë§¤ì¶œì•¡': 'revenue'
}

# ê´‘ê³ ìœ í˜• ê°’ ë§¤í•‘ (í•œê¸€ â†’ ì˜ë¬¸)
AD_TYPE_MAPPING = {
    'ë§¤ì¶œí˜•': 'sales',
    'ì ì¬ê³ ê°': 'lead'
}


def normalize_columns(df):
    """
    í•œê¸€/ì˜ë¬¸ ì»¬ëŸ¼ì„ ìë™ ê°ì§€í•˜ì—¬ ì˜ë¬¸ìœ¼ë¡œ í†µì¼

    Args:
        df: pandas DataFrame

    Returns:
        DataFrame: ì˜ë¬¸ ì»¬ëŸ¼ëª…ìœ¼ë¡œ í†µì¼ëœ DataFrame
    """
    # í•œê¸€ ì»¬ëŸ¼ëª…ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜
    rename_map = {}
    for kor, eng in COLUMN_MAPPING.items():
        if kor in df.columns:
            rename_map[kor] = eng

    if rename_map:
        df = df.rename(columns=rename_map)
        logger.info(f'Column mapping applied: {rename_map}')

    # ê´‘ê³ ìœ í˜• ê°’ ë³€í™˜ (ë§¤ì¶œí˜• â†’ sales, ì ì¬ê³ ê° â†’ lead)
    if 'ad_type' in df.columns:
        df['ad_type'] = df['ad_type'].map(lambda x: AD_TYPE_MAPPING.get(x, x) if pd.notna(x) else 'sales')
        logger.info(f'Ad type values mapped: {df["ad_type"].value_counts().to_dict()}')
    else:
        # ad_type ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 'sales' ì„¤ì •
        df['ad_type'] = 'sales'
        logger.info('No ad_type column found - defaulting to "sales"')

    return df



@ad_bp.before_app_request
def before_request():
    """
    ìš”ì²­ ì „ ì¸ì¦ ì²´í¬
    
    - ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì„¸ì…˜ ì²´í¬ë¥¼ ê±´ë„ˆëœ€
    - ì •ì  íŒŒì¼ ìš”ì²­ë„ ì„¸ì…˜ ì²´í¬ ì œì™¸
    """
    # ì •ì  íŒŒì¼ ë° ê³µê°œ í˜ì´ì§€ëŠ” ì„¸ì…˜ ì²´í¬ ì œì™¸
    if request.path.startswith('/static/') : return None
    if request.path.startswith('/landing'): return None

    # ì†Œì…œ ë¯¸ë””ì–´ ë´‡ì´ë©´ í™ˆ í˜ì´ì§€ ì„¸ì…˜ ì²´í¬ ê±´ë„ˆë›°ê¸° (OG ë©”íƒ€íƒœê·¸ìš©)
    user_agent = request.headers.get('User-Agent', '')
    if request.path == '/' and is_social_bot(user_agent):
        return None  # index()ì—ì„œ og_only.html ë°˜í™˜

    # ê°œë°œ ëª¨ë“œ ì²´í¬ (DEBUG ëª¨ë“œì´ê±°ë‚˜ FLASK_ENVê°€ developmentì¸ ê²½ìš°)
    is_debug_mode = current_app.config.get('DEBUG', False)
    flask_env = current_app.config.get('FLASK_ENV', os.getenv('FLASK_ENV', 'development'))
    is_development = flask_env == 'development' or is_debug_mode
    
    # ê°œë°œ ëª¨ë“œì´ë©´ ì„¸ì…˜ ì²´í¬ ê±´ë„ˆë›°ê¸°
    if is_development:
        logger.debug(f"[ê°œë°œ ëª¨ë“œ] ì„¸ì…˜ ì²´í¬ ê±´ë„ˆë›°ê¸°: {request.path}")
        g.user = {'userId': 'test', 'userNicknm': 'testNicknm'}
        return None
    
    # ìš´ì˜ ëª¨ë“œì—ì„œëŠ” ì„¸ì…˜ ì²´í¬ ìˆ˜í–‰
    COOKIE_VALUE = request.cookies.get('mbiz_session')
    SECRET_KEY = current_app.config.get('SECRET_KEY')
    SALT = 'cookie-session' # Flask ê¸°ë³¸ê°’

    serializer = URLSafeTimedSerializer(
        secret_key=SECRET_KEY,
        salt=SALT,
        serializer=flask.json.tag.TaggedJSONSerializer(),
        signer_kwargs={'key_derivation': 'hmac', 'digest_method': 'sha1'} 
    )

    try:
        data = serializer.loads(COOKIE_VALUE)
        if 'userId' in data :
            g.user = data
            print('g.user: ', g.user)
        else:
            g.user = {
                'userId': '',
                'name': '',
                'userNicknm': ''
            }
            if request.path == '/' : return None
            if request.path.startswith('/guide'): return None
            return redirect('https://mbizsquare.com/#/login')
    except Exception as e:
        print("âŒ ì‹¤íŒ¨! ì •í™•í•œ ì—ëŸ¬ ì›ì¸:", e)
        return redirect('https://mbizsquare.com/#/login')

# ========================================
# 1. ë©”ì¸ í˜ì´ì§€ ë° ì¸ì¦
# ========================================

@ad_bp.route('/landing')
def landing():
    """
    ëœë”©í˜ì´ì§€ (ê³µê°œ, ë¡œê·¸ì¸ ë¶ˆí•„ìš”)

    ì¿ íŒ¡ ê´‘ê³  ëŒ€ì‹œë³´ë“œ í™ë³´ ëœë”©í˜ì´ì§€
    ì¹´ì¹´ì˜¤í†¡ ì˜¤í”ˆì±„íŒ…ë°© ìœ ì… ëª©ì 

    Returns:
        HTML: ëœë”©í˜ì´ì§€
    """
    return send_from_directory('static/landing', 'index.html')


@ad_bp.route('/')
def index():
    """
    í™ˆ ëŒ€ì‹œë³´ë“œ (ë§ˆì¼€íŒ…ê´‘ì¥ ê´‘ê³ ë¶„ì„ ëŒ€ì‹œë³´ë“œ - ë§ˆê´‘)

    Returns:
        HTML: í™ˆ ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿
    """
    # ì†Œì…œ ë¯¸ë””ì–´ ë´‡(OG í¬ë¡¤ëŸ¬)ì´ë©´ ë©”íƒ€íƒœê·¸ë§Œ ìˆëŠ” í˜ì´ì§€ ë°˜í™˜
    user_agent = request.headers.get('User-Agent', '')
    if is_social_bot(user_agent):
        return render_template('og_only.html')

    # ë©”ì¸ í”„ë¡œì íŠ¸ ì„¸ì…˜ì—ì„œ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    user = get_current_user()

    return render_template('home_dashboard.html', user=user)


@ad_bp.route('/ad-dashboard')
def dashboard():
    """
    ê´‘ê³  ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë©”ì¸ í˜ì´ì§€ (ì¼ë°˜+ë©”íƒ€)

    Returns:
        HTML: ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿
    """
    # ë©”ì¸ í”„ë¡œì íŠ¸ ì„¸ì…˜ì—ì„œ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    user = get_current_user()

    return render_template('ad_dashboard_v2.html', user=user)


@ad_bp.route('/ad-dashboard/coupang-test')
def coupang_manual_test():
    """
    ì¿ íŒ¡ ê´‘ê³  ìˆ˜ë™ í…ŒìŠ¤íŠ¸ í˜ì´ì§€

    Returns:
        HTML: ìˆ˜ë™ í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿
    """
    return render_template('manual_test.html')


@ad_bp.route('/ad-dashboard/coupang')
@ad_bp.route('/ad-dashboard-coupang')  # ë³„ì¹­ ë¼ìš°íŠ¸ ì¶”ê°€
def coupang_dashboard():
    """
    ì¿ íŒ¡ ê´‘ê³  ì „ìš© ëŒ€ì‹œë³´ë“œ

    Returns:
        HTML: ì¿ íŒ¡ ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿
    """
    # ë©”ì¸ í”„ë¡œì íŠ¸ ì„¸ì…˜ì—ì„œ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    user = get_current_user()

    return render_template('ad_dashboard_coupang.html', user=user)


@ad_bp.route('/ad-dashboard/profit-simulator')
def profit_simulator():
    """
    ìˆ˜ìµ ì‹œë®¬ë ˆì´í„° í˜ì´ì§€

    Returns:
        HTML: ìˆ˜ìµ ì‹œë®¬ë ˆì´í„° í…œí”Œë¦¿
    """
    user = get_current_user()
    return render_template('profit_simulator.html', user=user)


@ad_bp.route('/ad-dashboard/ad-efficiency')
def ad_efficiency():
    """
    ê´‘ê³  íš¨ìœ¨ ì§„ë‹¨ í˜ì´ì§€

    Returns:
        HTML: ê´‘ê³  íš¨ìœ¨ ì§„ë‹¨ í…œí”Œë¦¿
    """
    user = get_current_user()
    return render_template('ad_efficiency.html', user=user)


@ad_bp.route('/ad-dashboard/keyword-combiner')
def keyword_combiner():
    """
    í‚¤ì›Œë“œ ì¡°í•©ê¸° í˜ì´ì§€

    Returns:
        HTML: í‚¤ì›Œë“œ ì¡°í•©ê¸° í…œí”Œë¦¿
    """
    user = get_current_user()
    return render_template('keyword_combiner.html', user=user)


@ad_bp.route('/guide')
def guide():
    """
    ì´ìš©ì•ˆë‚´ í˜ì´ì§€

    Returns:
        HTML: ì´ìš©ì•ˆë‚´ í…œí”Œë¦¿
    """
    return render_template('guide.html')


@ad_bp.route('/login')
def login():
    """
    ë¡œê·¸ì¸ í˜ì´ì§€ (JWT í† í° ì—†ì´ ì ‘ê·¼ ì‹œ)
    """
    return render_template('login.html')


@ad_bp.route('/logout')
def logout():
    """
    ë¡œê·¸ì•„ì›ƒ - ë©”ì¸ í”„ë¡œì íŠ¸ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    """
    session.clear()
    logger.info("User logged out")
    return redirect(current_app.config.get('MAIN_LOGIN_URL', 'https://mbizsquare.com/login'))


# ========================================
# 2. ë°ì´í„° ì—…ë¡œë“œ API
# ========================================

@ad_bp.route('/api/ad-analysis/upload', methods=['POST'])
def upload_data():
    """Excel/CSV íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥)"""
    user_id = get_current_user_id()

    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'}), 400

    file = request.files['file']

    if file.filename == '':
        return jsonify({'success': False, 'error': 'íŒŒì¼ëª…ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤'}), 400

    try:
        # íŒŒì¼ ì½ê¸°
        if file.filename.endswith('.csv'):
            df = pd.read_csv(file)
        else:
            # Excel íŒŒì¼ì¸ ê²½ìš°, ìë™ìœ¼ë¡œ ì ì ˆí•œ ì‹œíŠ¸ ì°¾ê¸°
            xl_file = pd.ExcelFile(file)

            # ì‹œíŠ¸ ìš°ì„ ìˆœìœ„: ì¼ë³„ë°ì´í„° > ê´‘ê³ ë°ì´í„° > ì²« ë²ˆì§¸ ì‹œíŠ¸
            if 'ì¼ë³„ë°ì´í„°' in xl_file.sheet_names:
                df = pd.read_excel(xl_file, sheet_name='ì¼ë³„ë°ì´í„°')
            elif 'ê´‘ê³ ë°ì´í„°' in xl_file.sheet_names:
                df = pd.read_excel(xl_file, sheet_name='ê´‘ê³ ë°ì´í„°')
            else:
                # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì½ê¸° (ì…ë ¥ì–‘ì‹ ì‹œíŠ¸ ìš°ì„ )
                if 'ì…ë ¥ì–‘ì‹' in xl_file.sheet_names:
                    df = pd.read_excel(xl_file, sheet_name='ì…ë ¥ì–‘ì‹')
                else:
                    df = pd.read_excel(xl_file, sheet_name=0)

        # ì»¬ëŸ¼ ì •ê·œí™” (í•œê¸€ â†’ ì˜ë¬¸ ë³€í™˜ + ê´‘ê³ ìœ í˜• ì²˜ë¦¬)
        df = normalize_columns(df)

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (ad_typeì€ normalize_columnsì—ì„œ ìë™ ì¶”ê°€ë¨)
        required_cols = ['date', 'campaign_name', 'spend', 'clicks', 'conversions', 'revenue']
        missing_cols = [col for col in required_cols if col not in df.columns]

        if missing_cols:
            # í•œê¸€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ
            kor_missing = [k for k, v in COLUMN_MAPPING.items() if v in missing_cols]
            return jsonify({'success': False, 'error': f'í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {kor_missing or missing_cols}'}), 400

        # Impression ë°ì´í„° ì²˜ë¦¬ (ì—†ê±°ë‚˜ 0ì´ë©´ ì¶”ì •)
        impressions_estimated = False
        if 'impressions' not in df.columns or df['impressions'].sum() == 0:
            # CTR 2% ê°€ì •í•˜ì—¬ ë…¸ì¶œìˆ˜ ì¶”ì •
            df['impressions'] = (df['clicks'] * 50).astype(int)
            impressions_estimated = True
            logger.info('Impressions column missing or zero - estimated from clicks (CTR ~2%)')

        # ìŠ¤ëƒ…ìƒ· ì´ë¦„ ìƒì„±
        snapshot_name = request.form.get('snapshot_name', f'ì—…ë¡œë“œ {pd.Timestamp.now().strftime("%Y-%m-%d %H:%M")}')

        # ì„ì‹œ ìŠ¤ëƒ…ìƒ· ID ìƒì„± (DB ëŒ€ì‹  ë©”ëª¨ë¦¬ ì‚¬ìš©)
        snapshot_id = int(pd.Timestamp.now().timestamp())

        # In-Memory ë°©ì‹ìœ¼ë¡œ ì§€í‘œ ê³„ì‚° (DB ì—†ì´)
        metrics = _calculate_metrics_inmemory(df)

        # Add impression estimation flag to metrics
        metrics['impressions_estimated'] = impressions_estimated

        # AI ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì„ íƒì‚¬í•­)
        try:
            ai = AIInsights()
            insights = ai.generate_insights(metrics, df)
        except Exception as ai_error:
            logger.warning(f'AI insights generation failed: {ai_error}')
            insights = 'âœ… ë¶„ì„ ì™„ë£Œ! ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.'

        # ì„¸ì…˜ì— ì €ì¥ (ì„ íƒì‚¬í•­)
        session[f'snapshot_{snapshot_id}'] = {
            'name': snapshot_name,
            'data': df.to_dict('records'),
            'metrics': metrics,
            'insights': insights,
            'created_at': pd.Timestamp.now().isoformat()
        }

        logger.info(f'File uploaded and processed in-memory: {file.filename}, snapshot_id: {snapshot_id}')

        return jsonify({
            'success': True,
            'snapshot_id': snapshot_id,
            'metrics': metrics,
            'insights': insights
        })

    except Exception as e:
        logger.error(f'File upload failed: {e}')
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'}), 500


@ad_bp.route('/api/ad-analysis/upload-coupang', methods=['POST'])
def upload_coupang():
    """
    ì¿ íŒ¡ ê´‘ê³  Excel íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹±

    ì¿ íŒ¡ ê´‘ê³  ë³´ê³ ì„œ í•„ìˆ˜ ì»¬ëŸ¼:
    - í‚¤ì›Œë“œ, ë…¸ì¶œìˆ˜, í´ë¦­ìˆ˜, ê´‘ê³ ë¹„, í´ë¦­ë¥ 
    - ì´ ì£¼ë¬¸ìˆ˜(1ì¼), ì´ íŒë§¤ìˆ˜ëŸ‰(1ì¼), ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼)
    - ì´ê´‘ê³ ìˆ˜ìµë¥ (1ì¼) = ROAS
    """
    user_id = get_current_user_id()

    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'}), 400

    file = request.files['file']

    if file.filename == '':
        return jsonify({'success': False, 'error': 'íŒŒì¼ëª…ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤'}), 400

    try:
        # Excel íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ë¬¸ì œ í•´ê²° - BytesIO ì‚¬ìš©)
        import io
        file_content = file.read()
        df = pd.read_excel(io.BytesIO(file_content), engine='openpyxl')
        logger.info(f'Coupang file uploaded: {file.filename}, rows: {len(df)}, columns: {len(df.columns)}')

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (ë§¤ì¶œì•¡ì€ 14ì¼ ìš°ì„ , ì—†ìœ¼ë©´ 1ì¼ ì‚¬ìš©)
        required_cols_base = ['í‚¤ì›Œë“œ', 'ë…¸ì¶œìˆ˜', 'í´ë¦­ìˆ˜', 'ê´‘ê³ ë¹„', 'í´ë¦­ë¥ ']

        # ê²½ê³  ë©”ì‹œì§€ ì´ˆê¸°í™”
        warning_message = None
        data_type = '14ì¼'

        # ë§¤ì¶œì•¡ ì»¬ëŸ¼ ì„ íƒ: 14ì¼ ìš°ì„ , ì—†ìœ¼ë©´ 1ì¼ ì‚¬ìš©
        if 'ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼)' in df.columns:
            revenue_col = 'ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼)'
            data_type = '14ì¼'
            logger.info('Using 14-day revenue data')
        elif 'ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼)' in df.columns:
            revenue_col = 'ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼)'
            data_type = '1ì¼'
            warning_message = 'âš ï¸ ì£¼ì˜: 14ì¼ ë°ì´í„°ê°€ ì—†ì–´ 1ì¼ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ROASê°€ ì‹¤ì œë³´ë‹¤ ë‚®ê²Œ í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
            logger.warning('Using 1-day revenue data (14-day not available)')
        else:
            logger.error('No revenue column found')
            return jsonify({'success': False, 'error': 'ë§¤ì¶œì•¡ ì»¬ëŸ¼ ì—†ìŒ (ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼) ë˜ëŠ” ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼) í•„ìš”)'}), 400

        # ROAS ì»¬ëŸ¼ ì„ íƒ
        if 'ì´ê´‘ê³ ìˆ˜ìµë¥ (14ì¼)' in df.columns:
            roas_col = 'ì´ê´‘ê³ ìˆ˜ìµë¥ (14ì¼)'
        elif 'ì´ê´‘ê³ ìˆ˜ìµë¥ (1ì¼)' in df.columns:
            roas_col = 'ì´ê´‘ê³ ìˆ˜ìµë¥ (1ì¼)'
        else:
            roas_col = None

        # ì£¼ë¬¸ìˆ˜/íŒë§¤ìˆ˜ëŸ‰ ì»¬ëŸ¼ ì„ íƒ
        if 'ì´ ì£¼ë¬¸ìˆ˜(14ì¼)' in df.columns:
            order_col = 'ì´ ì£¼ë¬¸ìˆ˜(14ì¼)'
            quantity_col = 'ì´ íŒë§¤ìˆ˜ëŸ‰(14ì¼)' if 'ì´ íŒë§¤ìˆ˜ëŸ‰(14ì¼)' in df.columns else 'ì´ íŒë§¤ìˆ˜ëŸ‰(1ì¼)'
        else:
            order_col = 'ì´ ì£¼ë¬¸ìˆ˜(1ì¼)'
            quantity_col = 'ì´ íŒë§¤ìˆ˜ëŸ‰(1ì¼)'

        required_cols = required_cols_base + [order_col, quantity_col, revenue_col]
        if roas_col:
            required_cols.append(roas_col)

        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            logger.error(f'Missing required columns: {missing}')
            return jsonify({'success': False, 'error': f'í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}'}), 400

        # ì»¬ëŸ¼ëª… í†µì¼ (14ì¼/1ì¼ ìƒê´€ì—†ì´ ë™ì¼í•œ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©)
        df = df.rename(columns={
            revenue_col: 'ì´ ì „í™˜ë§¤ì¶œì•¡',
            order_col: 'ì´ ì£¼ë¬¸ìˆ˜',
            quantity_col: 'ì´ íŒë§¤ìˆ˜ëŸ‰'
        })
        if roas_col:
            df = df.rename(columns={roas_col: 'ì´ê´‘ê³ ìˆ˜ìµë¥ '})

        logger.info(f'Column mapping: revenue={revenue_col}, orders={order_col}')

        # í‚¤ì›Œë“œ ì •ê·œí™”: ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µì¼
        if 'í‚¤ì›Œë“œ' in df.columns:
            original_keywords = df['í‚¤ì›Œë“œ'].nunique()
            df['í‚¤ì›Œë“œ'] = df['í‚¤ì›Œë“œ'].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()
            normalized_keywords = df['í‚¤ì›Œë“œ'].nunique()
            if original_keywords != normalized_keywords:
                logger.info(f'Keyword normalization: {original_keywords} â†’ {normalized_keywords} unique keywords')

        # ë°ì´í„° ì •ì œ
        # 1. ëª¨ë“  ê´‘ê³  ë…¸ì¶œ ì§€ë©´ ë°ì´í„° í¬í•¨ (ê²€ìƒ‰ì˜ì—­ + ë¹„ê²€ìƒ‰ì˜ì—­ + ë¦¬íƒ€ê²ŸíŒ…)
        # í‚¤ì›Œë“œê°€ '-'ì—¬ë„ í¬í•¨ (ë¹„ê²€ìƒ‰ì˜ì—­, ë¦¬íƒ€ê²ŸíŒ…ì˜ í‚¤ì›Œë“œëŠ” '-'ì„)
        if 'ê´‘ê³  ë…¸ì¶œ ì§€ë©´' in df.columns:
            exposure_types = df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'].value_counts()
            logger.info(f'Ad exposure types included: {exposure_types.to_dict()}')

        # ğŸ”¥ ë¹„ê²€ìƒ‰ì˜ì—­ ë° ë¦¬íƒ€ê²ŸíŒ… í†µí•© ì²˜ë¦¬
        if 'ê´‘ê³  ë…¸ì¶œ ì§€ë©´' in df.columns:
            # 1) ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•©
            non_search_mask = df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'].str.contains('ë¹„ê²€ìƒ‰', na=False)
            # 2) ë¦¬íƒ€ê²ŸíŒ… í†µí•©
            retargeting_mask = df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'].str.contains('ë¦¬íƒ€ê²ŸíŒ…', na=False)

            # ê²€ìƒ‰ì˜ì—­ë§Œ ë‚¨ê¹€ (ë¹„ê²€ìƒ‰, ë¦¬íƒ€ê²ŸíŒ… ì œì™¸)
            search_only_df = df[~(non_search_mask | retargeting_mask)].copy()

            aggregated_rows = []

            # === ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•© ===
            if non_search_mask.sum() > 0:
                non_search_df = df[non_search_mask].copy()
                logger.info(f'ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•© ì „: {non_search_mask.sum()}ê°œ í–‰')

                # ë¹„ê²€ìƒ‰ì˜ì—­ ì§€í‘œ í•©ì‚°
                non_search_aggregated = {
                    'í‚¤ì›Œë“œ': 'ë¹„ê²€ìƒ‰ì˜ì—­ (í†µí•©)',
                    'ê´‘ê³  ë…¸ì¶œ ì§€ë©´': 'ë¹„ê²€ìƒ‰ì˜ì—­ (í†µí•©)',
                    'ë…¸ì¶œìˆ˜': non_search_df['ë…¸ì¶œìˆ˜'].sum(),
                    'í´ë¦­ìˆ˜': non_search_df['í´ë¦­ìˆ˜'].sum(),
                    'ê´‘ê³ ë¹„': non_search_df['ê´‘ê³ ë¹„'].sum(),
                    'ì´ ì£¼ë¬¸ìˆ˜': non_search_df['ì´ ì£¼ë¬¸ìˆ˜'].sum(),
                    'ì´ íŒë§¤ìˆ˜ëŸ‰': non_search_df['ì´ íŒë§¤ìˆ˜ëŸ‰'].sum(),
                    'ì´ ì „í™˜ë§¤ì¶œì•¡': non_search_df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()
                }

                # í´ë¦­ë¥  ì¬ê³„ì‚°
                if non_search_aggregated['ë…¸ì¶œìˆ˜'] > 0:
                    non_search_aggregated['í´ë¦­ë¥ '] = (non_search_aggregated['í´ë¦­ìˆ˜'] / non_search_aggregated['ë…¸ì¶œìˆ˜']) * 100
                else:
                    non_search_aggregated['í´ë¦­ë¥ '] = 0

                # ROAS ì¬ê³„ì‚° (ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ Excel ë°ì´í„°ì™€ ì¼ì¹˜)
                if non_search_aggregated['ê´‘ê³ ë¹„'] > 0:
                    roas_value = (non_search_aggregated['ì´ ì „í™˜ë§¤ì¶œì•¡'] / non_search_aggregated['ê´‘ê³ ë¹„']) * 100
                    non_search_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = f"{roas_value:.2f}%"
                else:
                    non_search_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = "0.00%"

                aggregated_rows.append(non_search_aggregated)
                logger.info(f'ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•© ì™„ë£Œ: 1ê°œ í–‰ìœ¼ë¡œ í†µí•©ë¨')
            else:
                logger.info('ë¹„ê²€ìƒ‰ì˜ì—­ ë°ì´í„° ì—†ìŒ')

            # === ë¦¬íƒ€ê²ŸíŒ… í†µí•© ===
            if retargeting_mask.sum() > 0:
                retargeting_df = df[retargeting_mask].copy()
                logger.info(f'ë¦¬íƒ€ê²ŸíŒ… í†µí•© ì „: {retargeting_mask.sum()}ê°œ í–‰')

                # ë¦¬íƒ€ê²ŸíŒ… ì§€í‘œ í•©ì‚°
                retargeting_aggregated = {
                    'í‚¤ì›Œë“œ': 'ë¦¬íƒ€ê²ŸíŒ… (í†µí•©)',
                    'ê´‘ê³  ë…¸ì¶œ ì§€ë©´': 'ë¦¬íƒ€ê²ŸíŒ… (í†µí•©)',
                    'ë…¸ì¶œìˆ˜': retargeting_df['ë…¸ì¶œìˆ˜'].sum(),
                    'í´ë¦­ìˆ˜': retargeting_df['í´ë¦­ìˆ˜'].sum(),
                    'ê´‘ê³ ë¹„': retargeting_df['ê´‘ê³ ë¹„'].sum(),
                    'ì´ ì£¼ë¬¸ìˆ˜': retargeting_df['ì´ ì£¼ë¬¸ìˆ˜'].sum(),
                    'ì´ íŒë§¤ìˆ˜ëŸ‰': retargeting_df['ì´ íŒë§¤ìˆ˜ëŸ‰'].sum(),
                    'ì´ ì „í™˜ë§¤ì¶œì•¡': retargeting_df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()
                }

                # í´ë¦­ë¥  ì¬ê³„ì‚°
                if retargeting_aggregated['ë…¸ì¶œìˆ˜'] > 0:
                    retargeting_aggregated['í´ë¦­ë¥ '] = (retargeting_aggregated['í´ë¦­ìˆ˜'] / retargeting_aggregated['ë…¸ì¶œìˆ˜']) * 100
                else:
                    retargeting_aggregated['í´ë¦­ë¥ '] = 0

                # ROAS ì¬ê³„ì‚° (ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ Excel ë°ì´í„°ì™€ ì¼ì¹˜)
                if retargeting_aggregated['ê´‘ê³ ë¹„'] > 0:
                    roas_value = (retargeting_aggregated['ì´ ì „í™˜ë§¤ì¶œì•¡'] / retargeting_aggregated['ê´‘ê³ ë¹„']) * 100
                    retargeting_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = f"{roas_value:.2f}%"
                else:
                    retargeting_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = "0.00%"

                aggregated_rows.append(retargeting_aggregated)
                logger.info(f'ë¦¬íƒ€ê²ŸíŒ… í†µí•© ì™„ë£Œ: 1ê°œ í–‰ìœ¼ë¡œ í†µí•©ë¨')
            else:
                logger.info('ë¦¬íƒ€ê²ŸíŒ… ë°ì´í„° ì—†ìŒ')

            # í†µí•©ëœ ë°ì´í„° ë³‘í•©
            if aggregated_rows:
                aggregated_df = pd.DataFrame(aggregated_rows)
                df = pd.concat([search_only_df, aggregated_df], ignore_index=True)
            else:
                df = search_only_df

        logger.info(f'Total keywords to analyze (before dedup): {len(df)}ê°œ')

        # ğŸ”¥ í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° - ë™ì¼ í‚¤ì›Œë“œëŠ” ë°ì´í„° í•©ì‚°
        if 'í‚¤ì›Œë“œ' in df.columns:
            keyword_groups = df.groupby('í‚¤ì›Œë“œ', as_index=False).agg({
                'ë…¸ì¶œìˆ˜': 'sum',
                'í´ë¦­ìˆ˜': 'sum',
                'ê´‘ê³ ë¹„': 'sum',
                'ì´ ì£¼ë¬¸ìˆ˜': 'sum',
                'ì´ íŒë§¤ìˆ˜ëŸ‰': 'sum',
                'ì´ ì „í™˜ë§¤ì¶œì•¡': 'sum',
                'ê´‘ê³  ë…¸ì¶œ ì§€ë©´': 'first',  # ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            })

            # í´ë¦­ë¥  ì¬ê³„ì‚° (Infinity ë°©ì§€)
            keyword_groups['í´ë¦­ë¥ '] = (keyword_groups['í´ë¦­ìˆ˜'] / keyword_groups['ë…¸ì¶œìˆ˜'] * 100).replace([np.inf, -np.inf], 0).fillna(0)

            # ROAS ì¬ê³„ì‚°
            keyword_groups['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = keyword_groups.apply(
                lambda row: f"{(row['ì´ ì „í™˜ë§¤ì¶œì•¡'] / row['ê´‘ê³ ë¹„'] * 100):.2f}%" if row['ê´‘ê³ ë¹„'] > 0 else "0.00%",
                axis=1
            )

            df = keyword_groups
            logger.info(f'Keyword deduplication completed: {len(df)}ê°œ (unique keywords)')

        # 2. í´ë¦­ë¥  ì²˜ë¦¬ (ì´ë¯¸ % í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ, ì†Œìˆ˜ì ì´ë©´ 100 ê³±í•˜ê¸°)
        if df['í´ë¦­ë¥ '].max() <= 1:
            df['í´ë¦­ë¥ '] = df['í´ë¦­ë¥ '] * 100

        # 3. ROAS íŒŒì‹±
        if df['ì´ê´‘ê³ ìˆ˜ìµë¥ '].dtype == 'object':
            # "356.78%" â†’ 356.78 ë³€í™˜
            df['ROAS'] = df['ì´ê´‘ê³ ìˆ˜ìµë¥ '].str.rstrip('%').astype(float)
        else:
            df['ROAS'] = df['ì´ê´‘ê³ ìˆ˜ìµë¥ ']
            if df['ROAS'].max() <= 10:  # ì†Œìˆ˜ì  í˜•ì‹ (3.56 â†’ 356)
                df['ROAS'] = df['ROAS'] * 100

        # 4. CPC ê³„ì‚° (í´ë¦­ë‹¹ ë‹¨ê°€) - Infinity ë°©ì§€
        df['CPC'] = (df['ê´‘ê³ ë¹„'] / df['í´ë¦­ìˆ˜']).replace([np.inf, -np.inf], 0).fillna(0)

        # 5. ê²°ì¸¡ì¹˜ ë° Infinity ì²˜ë¦¬ (JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            df[col] = df[col].replace([np.inf, -np.inf], 0).fillna(0)

        logger.info(f'Processed {len(df)} valid keywords')

        # ìš”ì•½ ì§€í‘œ ê³„ì‚°
        total_spend = df['ê´‘ê³ ë¹„'].sum()
        total_revenue = df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()

        # í‰ê· CTR ê³„ì‚° (Infinity ë°©ì§€)
        avg_ctr = df['í´ë¦­ë¥ '].mean()
        if np.isinf(avg_ctr) or np.isnan(avg_ctr):
            avg_ctr = 0

        summary = {
            'ì´ê´‘ê³ ë¹„': int(total_spend),
            'ì´ë§¤ì¶œì•¡': int(total_revenue),
            'í‰ê· ROAS': round((total_revenue / total_spend * 100), 2) if total_spend > 0 else 0,
            'ì´í´ë¦­ìˆ˜': int(df['í´ë¦­ìˆ˜'].sum()),
            'í‰ê· CTR': round(float(avg_ctr), 2),
            'ì´ë…¸ì¶œìˆ˜': int(df['ë…¸ì¶œìˆ˜'].sum()),
            'ì´ì£¼ë¬¸ìˆ˜': int(df['ì´ ì£¼ë¬¸ìˆ˜'].sum())
        }

        # JSON ì•ˆì „ ë³€í™˜ í•¨ìˆ˜
        import math
        def sanitize_for_json(obj):
            """Infinity, -Infinity, NaNì„ JSON ì•ˆì „ ê°’ìœ¼ë¡œ ë³€í™˜"""
            if isinstance(obj, (float, np.floating)):
                if math.isinf(float(obj)) or math.isnan(float(obj)):
                    return 0
                return float(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            return obj

        # DataFrameì„ dictë¡œ ë³€í™˜ í›„ Infinity/NaN ì²˜ë¦¬
        data = df.to_dict('records')
        for row in data:
            for key, value in row.items():
                row[key] = sanitize_for_json(value)

        # ì„¸ì…˜ì— ì €ì¥ (ì„ íƒì‚¬í•­)
        snapshot_id = int(pd.Timestamp.now().timestamp())
        session[f'coupang_snapshot_{snapshot_id}'] = {
            'data': data,
            'summary': summary,
            'created_at': pd.Timestamp.now().isoformat()
        }

        logger.info(f'Coupang data processed successfully: {len(data)} keywords')

        # JSON ì‘ë‹µ ìƒì„±
        response_data = {
            'success': True,
            'data': data,
            'summary': summary,
            'data_type': data_type
        }

        # ê²½ê³  ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ í¬í•¨
        if warning_message:
            response_data['warning'] = warning_message

        return jsonify(response_data)

    except Exception as e:
        logger.error(f'Coupang file upload failed: {e}')
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'}), 500


@ad_bp.route('/api/ad-analysis/coupang-recommendations', methods=['POST'])
def coupang_recommendations():
    """
    ì¿ íŒ¡ ê´‘ê³  í‚¤ì›Œë“œ ì œì™¸ ì¶”ì²œ (í–¥ìƒëœ 0-100ì  ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ)

    Request Body:
        {
            "data": [...],  # í‚¤ì›Œë“œ ë°ì´í„°
            "criteria": {
                "target_roas": 400  # ëª©í‘œ ROAS (ê¸°ë³¸ê°’: 400%)
            }
        }

    Response:
        {
            "success": true,
            "recommendations": [
                {
                    "keyword": "í‚¤ì›Œë“œëª…",
                    "score": 85,
                    "priority": "critical",
                    "reason": "ì¦‰ì‹œ ì œì™¸ - ROAS 15%, ì „í™˜ 0ì›",
                    "spend": 1200,
                    "revenue": 0,
                    "roas": 0,
                    "waste": 1200,
                    "waste_rate": 100,
                    "opportunity_loss": 4800,
                    "clicks": 10,
                    "ctr": 2.5,
                    "cpc": 120
                }
            ],
            "summary": {
                "total_waste": 39447,
                "total_opportunity_loss": 157788,
                "keywords_to_exclude": 182,
                "potential_savings": "94.0%",
                "critical_priority": 150,
                "high_priority": 32,
                "medium_priority": 0,
                "low_priority": 0
            }
        }
    """
    try:
        data = request.get_json()
        keywords = data.get('data', [])
        criteria = data.get('criteria', {})

        if not keywords:
            return jsonify({'success': False, 'error': 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400

        df = pd.DataFrame(keywords)
        logger.info(f'Analyzing {len(df)} keywords with enhanced scoring system')

        # ===== ì¤‘ìš”: ê²€ìƒ‰ì˜ì—­ë§Œ ì¶”ì²œ ëŒ€ìƒìœ¼ë¡œ ë¶„ì„ =====
        # ë¹„ê²€ìƒ‰ì˜ì—­, ë¦¬íƒ€ê²ŸíŒ…ì€ ì¶”ì²œì—ì„œ ì œì™¸
        if 'ê´‘ê³  ë…¸ì¶œ ì§€ë©´' in df.columns:
            original_count = len(df)
            df = df[df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'] == 'ê²€ìƒ‰ ì˜ì—­'].copy()
            logger.info(f'Filtered to search area only: {len(df)} keywords (from {original_count})')

        if len(df) == 0:
            return jsonify({
                'success': True,
                'recommendations': [],
                'summary': {
                    'total_waste': 0,
                    'total_opportunity_loss': 0,
                    'keywords_to_exclude': 0,
                    'potential_savings': '0%',
                    'critical_priority': 0,
                    'high_priority': 0,
                    'medium_priority': 0,
                    'low_priority': 0
                }
            })

        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        total_spend = df['ê´‘ê³ ë¹„'].sum()
        total_revenue = df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()
        avg_roas = (total_revenue / total_spend * 100) if total_spend > 0 else 0
        target_roas = criteria.get('target_roas', 400)  # ëª©í‘œ ROAS 400%

        # === Phase 2: ì¤‘ì•™ê°’ ê¸°ë°˜ í†µê³„ (Robust Statistics) ===
        median_cpc = df['CPC'].median()
        median_ctr = df['í´ë¦­ë¥ '].median()

        # CPC ë°±ë¶„ìœ„ìˆ˜
        cpc_percentiles = {
            'p25': df['CPC'].quantile(0.25),
            'p50': df['CPC'].quantile(0.50),
            'p75': df['CPC'].quantile(0.75),
            'p90': df['CPC'].quantile(0.90)
        }

        # ì§€ì¶œì•¡ ë°±ë¶„ìœ„ìˆ˜
        spend_percentiles = {
            'p25': df['ê´‘ê³ ë¹„'].quantile(0.25),
            'p50': df['ê´‘ê³ ë¹„'].quantile(0.50),
            'p75': df['ê´‘ê³ ë¹„'].quantile(0.75),
            'p90': df['ê´‘ê³ ë¹„'].quantile(0.90)
        }

        # ì„±ê³¼ êµ¬ê°„ë³„ í†µê³„
        tier_stats = {}
        tier_definitions = {
            'elite': df[df['ROAS'] >= 500],
            'high': df[(df['ROAS'] >= 300) & (df['ROAS'] < 500)],
            'mid': df[(df['ROAS'] >= 150) & (df['ROAS'] < 300)],
            'low': df[df['ROAS'] < 150]
        }

        for tier_name, tier_df in tier_definitions.items():
            if len(tier_df) > 0:
                tier_stats[tier_name] = {
                    'median_cpc': tier_df['CPC'].median(),
                    'p75_cpc': tier_df['CPC'].quantile(0.75),
                    'count': len(tier_df),
                    'avg_roas': tier_df['ROAS'].mean()
                }

        # ìƒìœ„ ì„±ê³¼ í‚¤ì›Œë“œ ê¸°ì¤€ (ê¸°íšŒë¹„ìš© ê³„ì‚°ìš©)
        top_performers = df[df['ROAS'] >= target_roas]
        if len(top_performers) > 0:
            top_avg_roas = top_performers['ROAS'].mean()
        else:
            top_avg_roas = avg_roas

        recommendations = []

        for _, row in df.iterrows():
            keyword = row['í‚¤ì›Œë“œ']
            spend = float(row['ê´‘ê³ ë¹„'])
            revenue = float(row['ì´ ì „í™˜ë§¤ì¶œì•¡'])
            roas = float(row.get('ROAS', 0))
            clicks = int(row['í´ë¦­ìˆ˜'])
            ctr = float(row['í´ë¦­ë¥ '])
            cpc = float(row['CPC'])

            # === Phase 2: ìƒˆë¡œìš´ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ ===
            reasons = []

            # === 1. ìˆ˜ìµì„± ì ìˆ˜ (0-50ì ) - ROAS ê¸°ë°˜ ===
            if revenue == 0:
                profitability_score = 50
                reasons.append("ì „í™˜ 0ì›")
            elif roas < 20:
                profitability_score = 45
                reasons.append(f"ROAS {roas:.1f}% (ê·¹ì‹¬í•œ ì†ì‹¤)")
            elif roas < 50:
                profitability_score = 40
                reasons.append(f"ROAS {roas:.1f}% (ì‹¬ê°í•œ ì†ì‹¤)")
            elif roas < 100:
                profitability_score = 35
                reasons.append(f"ROAS {roas:.1f}% (ì†ì‹¤)")
            elif roas < 150:
                profitability_score = 25
                reasons.append(f"ROAS {roas:.1f}% (ë‚®ì€ ìˆ˜ìµ)")
            elif roas < 200:
                profitability_score = 15
                reasons.append(f"ROAS {roas:.1f}% (ëª©í‘œ ë¯¸ë‹¬)")
            elif roas < 300:
                profitability_score = 10
                reasons.append(f"ROAS {roas:.1f}% (ëª©í‘œ ê·¼ì ‘)")
            else:
                profitability_score = 0  # ROAS >= 300%

            # === 2. íš¨ìœ¨ì„± ì ìˆ˜ (0-25ì ) - ì„±ê³¼ êµ¬ê°„ë³„ CPC ë¹„êµ ===
            # í‚¤ì›Œë“œ ì„±ê³¼ êµ¬ê°„ íŒì •
            if roas >= 500:
                tier = 'elite'
            elif roas >= 300:
                tier = 'high'
            elif roas >= 150:
                tier = 'mid'
            else:
                tier = 'low'

            # í•´ë‹¹ êµ¬ê°„ì˜ ì¤‘ì•™ê°’ CPC
            if tier in tier_stats and tier_stats[tier]['count'] >= 3:
                tier_median_cpc = tier_stats[tier]['median_cpc']
            else:
                tier_median_cpc = median_cpc  # fallback

            # CPC ë¹„ìœ¨ ê³„ì‚°
            if tier_median_cpc > 0:
                cpc_ratio = cpc / tier_median_cpc
            else:
                cpc_ratio = 1.0

            # ì„±ê³¼ êµ¬ê°„ë³„ë¡œ ë‹¤ë¥¸ ê¸°ì¤€ ì ìš©
            if tier in ['elite', 'high']:
                # ê³ ì„±ê³¼ í‚¤ì›Œë“œ: CPC ê¸°ì¤€ ê´€ëŒ€
                if cpc_ratio > 3.0:
                    efficiency_score = 10
                    reasons.append(f"CPC ê³¼ë‹¤ ({cpc:.0f}ì›)")
                elif cpc_ratio > 2.5:
                    efficiency_score = 5
                else:
                    efficiency_score = 0
            elif tier == 'mid':
                # ì¤‘ì„±ê³¼ í‚¤ì›Œë“œ: ë³´í†µ ê¸°ì¤€
                if cpc_ratio > 2.5:
                    efficiency_score = 20
                    reasons.append(f"CPC ë†’ìŒ ({cpc:.0f}ì›)")
                elif cpc_ratio > 2.0:
                    efficiency_score = 15
                elif cpc_ratio > 1.5:
                    efficiency_score = 10
                else:
                    efficiency_score = 0
            else:
                # ì €ì„±ê³¼ í‚¤ì›Œë“œ: CPC ê¸°ì¤€ ì—„ê²©
                if cpc_ratio > 2.0:
                    efficiency_score = 25
                    reasons.append(f"CPC ê³¼ë‹¤ ({cpc:.0f}ì›)")
                elif cpc_ratio > 1.5:
                    efficiency_score = 20
                elif cpc_ratio > 1.2:
                    efficiency_score = 15
                else:
                    efficiency_score = 5

            # === 3. ê·œëª¨ ë¦¬ìŠ¤í¬ ì ìˆ˜ (0-25ì ) - ì§€ì¶œì•¡ + ROAS ì¡°í•© ===
            # ì§€ì¶œ ìˆ˜ì¤€ íŒì •
            if spend > spend_percentiles['p90']:
                spend_level = 'very_high'
            elif spend > spend_percentiles['p75']:
                spend_level = 'high'
            elif spend > spend_percentiles['p50']:
                spend_level = 'medium'
            else:
                spend_level = 'low'

            # ROASì™€ ì§€ì¶œ ì¡°í•©ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
            if roas == 0:
                # ì „í™˜ 0ì› ì¼€ì´ìŠ¤
                if spend_level == 'very_high':
                    scale_risk_score = 25
                    reasons.append(f"ê³ ì§€ì¶œ ({spend:,.0f}ì›)")
                elif spend_level == 'high':
                    scale_risk_score = 20
                    reasons.append(f"ì¤‘ê°„ ì§€ì¶œ")
                elif spend_level == 'medium':
                    scale_risk_score = 15
                else:
                    scale_risk_score = 10
            elif roas < 100:
                # ì†ì‹¤ ì¼€ì´ìŠ¤
                if spend_level == 'very_high':
                    scale_risk_score = 20
                    reasons.append(f"ê³ ì§€ì¶œ ({spend:,.0f}ì›)")
                elif spend_level == 'high':
                    scale_risk_score = 15
                elif spend_level == 'medium':
                    scale_risk_score = 10
                else:
                    scale_risk_score = 5
            elif roas < 200:
                # ë‚®ì€ ìˆ˜ìµ ì¼€ì´ìŠ¤
                if spend_level in ['very_high', 'high']:
                    scale_risk_score = 10
                else:
                    scale_risk_score = 0
            elif roas < 300:
                # ëª©í‘œ ë¯¸ë‹¬ ì¼€ì´ìŠ¤
                if spend_level == 'very_high':
                    scale_risk_score = 5
                else:
                    scale_risk_score = 0
            else:
                # ëª©í‘œ ë‹¬ì„± (ROAS >= 300%)
                scale_risk_score = 0

            # ì´ì  ê³„ì‚°
            score = profitability_score + efficiency_score + scale_risk_score

            # === Phase 3: ê°œì„ ëœ ìš°ì„ ìˆœìœ„ ê²°ì • (ì¡°ê±´ ê¸°ë°˜) ===
            # 0ë‹¨ê³„: ë°ì´í„° ë¶€ì¡± íŒì •
            if spend < EXCLUDE_MIN_SPEND and clicks < EXCLUDE_MIN_CLICKS:
                priority = None
                priority_label = 'ë°ì´í„° ë¶€ì¡±'
                reasons.append('ë°ì´í„° ë¶€ì¡±')
            # 1ë‹¨ê³„: ê³ CPC + ì €ROAS â†’ ì¦‰ì‹œì œì™¸ (ê´‘ê³ ë¹„ ë¬´ê´€)
            elif cpc >= EXCLUDE_CPC_CRITICAL and roas < 100:
                priority = 'critical'
                priority_label = 'ì¦‰ì‹œ ì œì™¸'
                reasons.append(f'ê³ CPC({cpc:.0f}ì›) + ì €ROAS')
            elif cpc >= EXCLUDE_CPC_VERY_HIGH and roas < 200:
                priority = 'critical'
                priority_label = 'ì¦‰ì‹œ ì œì™¸'
                reasons.append(f'ì´ˆê³ CPC({cpc:.0f}ì›) + ì €ROAS')
            # 2ë‹¨ê³„: ì „í™˜ì—†ìŒ (ROAS 0%)
            elif revenue == 0:
                if clicks >= EXCLUDE_CLICKS_CRITICAL:
                    priority = 'critical'
                    priority_label = 'ì¦‰ì‹œ ì œì™¸'
                    reasons.append(f'{clicks}í´ë¦­ ì „í™˜ì—†ìŒ')
                elif clicks >= EXCLUDE_CLICKS_HIGH:
                    priority = 'high'
                    priority_label = 'ì¡°ì†íˆ ì œì™¸'
                    reasons.append(f'{clicks}í´ë¦­ ì „í™˜ì—†ìŒ')
                else:
                    priority = 'medium'
                    priority_label = 'ê²€í†  í•„ìš”'
                    reasons.append('ì „í™˜ì—†ìŒ ê²€í† ')
            # 3ë‹¨ê³„: ROAS 1~100% (ì†ì‹¤)
            elif roas < 100:
                if spend >= spend_percentiles['p75']:
                    priority = 'critical'
                    priority_label = 'ì¦‰ì‹œ ì œì™¸'
                    reasons.append('ì €ROAS + ê³ ì§€ì¶œ')
                elif spend >= spend_percentiles['p50']:
                    priority = 'high'
                    priority_label = 'ì¡°ì†íˆ ì œì™¸'
                    reasons.append('ì €ROAS + ì¤‘ì§€ì¶œ')
                else:
                    priority = 'medium'
                    priority_label = 'ê²€í†  í•„ìš”'
            # 4ë‹¨ê³„: ROAS 100~200% (ì €ì¡°)
            elif roas < 200:
                if spend >= spend_percentiles['p75']:
                    priority = 'high'
                    priority_label = 'ì¡°ì†íˆ ì œì™¸'
                    reasons.append('ì €ì¡°ROAS + ê³ ì§€ì¶œ')
                else:
                    priority = 'medium'
                    priority_label = 'ê²€í†  í•„ìš”'
            # 5ë‹¨ê³„: ROAS 200~300% (ëª©í‘œ ê·¼ì ‘)
            elif roas < 300:
                priority = 'medium'
                priority_label = 'ê²€í†  í•„ìš”'
                reasons.append('ROAS ê°œì„ í•„ìš”')
            # 6ë‹¨ê³„: ROAS 300%+ (ì–‘í˜¸)
            else:
                priority = 'low'
                priority_label = 'ëª¨ë‹ˆí„°ë§'

            # === ë‚­ë¹„ ë° ê¸°íšŒë¹„ìš© ê³„ì‚° ===
            if spend == 0:
                # ê´‘ê³ ë¹„ 0ì›ì¸ ê²½ìš° - ë¹„ì •ìƒ ë°ì´í„° (ë‚­ë¹„ ì—†ìŒ)
                waste = 0
                waste_rate = 0
                expected_revenue = 0
                opportunity_loss = 0
            elif roas < 100:
                # ì†ì‹¤ ì¼€ì´ìŠ¤: ê´‘ê³ ë¹„ - ë§¤ì¶œ
                waste = spend - revenue
                waste_rate = 100 - roas
                # ê¸°íšŒë¹„ìš©: ì´ ê´‘ê³ ë¹„ë¥¼ ìƒìœ„ ì„±ê³¼ í‚¤ì›Œë“œì— íˆ¬ìí–ˆì„ ë•Œì˜ ê¸°ëŒ€ ë§¤ì¶œ
                expected_revenue = spend * (top_avg_roas / 100)
                opportunity_loss = expected_revenue - revenue
            else:
                # ëª©í‘œ ë¯¸ë‹¬ ì¼€ì´ìŠ¤: ë‚­ë¹„ ì—†ìŒ
                waste = 0
                waste_rate = 0
                # ê¸°íšŒë¹„ìš©: ì´ ê´‘ê³ ë¹„ë¥¼ ìƒìœ„ ì„±ê³¼ í‚¤ì›Œë“œì— íˆ¬ìí–ˆì„ ë•Œì˜ ê¸°ëŒ€ ë§¤ì¶œ
                expected_revenue = spend * (top_avg_roas / 100)
                opportunity_loss = expected_revenue - revenue

            # === ì¶”ì²œ ì‚¬ìœ  ìƒì„± ===
            reason = f"{priority_label} - " + ", ".join(reasons[:3])  # ìµœëŒ€ 3ê°œ ì‚¬ìœ 

            recommendations.append({
                'keyword': keyword,
                'score': int(score),
                'priority': priority,
                'reason': reason,
                'spend': spend,
                'revenue': revenue,
                'roas': roas,
                'waste': float(waste),
                'waste_rate': float(waste_rate),
                'opportunity_loss': float(opportunity_loss),
                'clicks': clicks,
                'ctr': ctr,
                'cpc': cpc
            })

        # === ì •ë ¬: ì ìˆ˜ ë†’ì€ ìˆœ ===
        recommendations.sort(key=lambda x: -x['score'])

        # === ìš”ì•½ í†µê³„ ===
        total_waste = sum(r['waste'] for r in recommendations)
        total_opportunity_loss = sum(r['opportunity_loss'] for r in recommendations)

        # ë°ì´í„° ë¶€ì¡± í‚¤ì›Œë“œ ë¶„ë¦¬
        insufficient_data = [r for r in recommendations if r['priority'] is None]
        valid_recommendations = [r for r in recommendations if r['priority'] is not None]

        summary = {
            'total_waste': int(total_waste),
            'total_opportunity_loss': int(total_opportunity_loss),
            'keywords_to_exclude': len(valid_recommendations),
            'potential_savings': f"{(total_waste / total_spend * 100):.1f}%" if total_spend > 0 else "0%",
            'critical_priority': len([r for r in valid_recommendations if r['priority'] == 'critical']),
            'high_priority': len([r for r in valid_recommendations if r['priority'] == 'high']),
            'medium_priority': len([r for r in valid_recommendations if r['priority'] == 'medium']),
            'low_priority': len([r for r in valid_recommendations if r['priority'] == 'low']),
            'insufficient_data': len(insufficient_data),
            'avg_score': int(sum(r['score'] for r in valid_recommendations) / len(valid_recommendations)) if valid_recommendations else 0
        }

        logger.info(f'Generated {len(recommendations)} recommendations (avg score: {summary["avg_score"]}, total waste: {total_waste:.0f}ì›)')

        return jsonify({
            'success': True,
            'recommendations': recommendations,
            'summary': summary
        })

    except Exception as e:
        logger.error(f'Recommendation generation failed: {e}')
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}'}), 500


@ad_bp.route('/api/ad-analysis/manual-input', methods=['POST'])
def manual_input():
    """ìˆ˜ê¸° ë°ì´í„° ì…ë ¥ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥)"""
    user_id = get_current_user_id()

    try:
        data = request.get_json()

        if not data or 'data' not in data:
            return jsonify({'success': False, 'error': 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400

        # DataFrame ìƒì„±
        df = pd.DataFrame(data['data'])

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['date', 'campaign_name', 'spend', 'clicks', 'conversions', 'revenue']
        missing_cols = [col for col in required_cols if col not in df.columns]

        if missing_cols:
            return jsonify({
                'success': False,
                'error': f'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {", ".join(missing_cols)}'
            }), 400

        snapshot_name = data.get('snapshot_name', f'ìˆ˜ê¸°ì…ë ¥ {pd.Timestamp.now().strftime("%Y-%m-%d %H:%M")}')

        # ì„ì‹œ ìŠ¤ëƒ…ìƒ· ID ìƒì„± (DB ëŒ€ì‹  ë©”ëª¨ë¦¬ ì‚¬ìš©)
        snapshot_id = int(pd.Timestamp.now().timestamp())

        # In-Memory ë°©ì‹ìœ¼ë¡œ ì§€í‘œ ê³„ì‚° (DB ì—†ì´)
        metrics = _calculate_metrics_inmemory(df)

        # ì„¸ì…˜ì— ì €ì¥ (ì„ íƒì‚¬í•­)
        session[f'snapshot_{snapshot_id}'] = {
            'name': snapshot_name,
            'data': df.to_dict('records'),
            'metrics': metrics,
            'created_at': pd.Timestamp.now().isoformat()
        }

        logger.info(f'Manual data input processed in-memory: {len(df)} rows, snapshot_id: {snapshot_id}')

        return jsonify({
            'success': True,
            'snapshot_id': snapshot_id,
            'metrics': metrics
        })

    except Exception as e:
        logger.error(f'Manual input failed: {e}')
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'}), 500


# ========================================
# 3. ë¶„ì„ ê´€ë¦¬ API
# ========================================

@ad_bp.route('/api/ad-analysis/snapshots')
def get_snapshots():
    """
    ì €ì¥ëœ ë¶„ì„ ëª©ë¡ ì¡°íšŒ

    Query Params:
        - saved_only: true/false (ì €ì¥ëœ ê²ƒë§Œ)

    Response:
        {
            "snapshots": [...]
        }
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id
    saved_only = request.args.get('saved_only', 'false').lower() == 'true'

    try:
        analyzer = AdAnalyzer(user_id)
        snapshots = analyzer.get_snapshots(saved_only)

        return jsonify({'snapshots': snapshots})

    except Exception as e:
        logger.error(f"Get snapshots failed: {e}")
        return create_error_response("ë¶„ì„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨", 500)


@ad_bp.route('/api/ad-analysis/snapshots/<int:snapshot_id>')
def get_snapshot_detail(snapshot_id):
    """
    íŠ¹ì • ë¶„ì„ ìƒì„¸ ì¡°íšŒ

    Response:
        {
            "snapshot": {...},
            "daily_data": [...],
            "metrics": {...},
            "insights": "...",
            "campaigns": [...]
        }
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    try:
        analyzer = AdAnalyzer(user_id)

        # ì†Œìœ ê¶Œ í™•ì¸
        if not analyzer.check_ownership(snapshot_id):
            return create_error_response("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤", 403)

        data = analyzer.get_snapshot_detail(snapshot_id)

        return jsonify(data)

    except ValueError as e:
        return create_error_response(str(e), 404)

    except Exception as e:
        logger.error(f"Get snapshot detail failed: {e}")
        return create_error_response("ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨", 500)


@ad_bp.route('/api/ad-analysis/snapshots/<int:snapshot_id>', methods=['PUT'])
def update_snapshot(snapshot_id):
    """
    ë¶„ì„ ì €ì¥/ìˆ˜ì •

    Request Body:
        {
            "is_saved": true,
            "snapshot_name": "ìˆ˜ì •ëœ ì´ë¦„",
            "tags": "ë¸”í”„,ì‹ ê·œ",
            "memo": "ë©”ëª¨ ë‚´ìš©"
        }

    Response:
        {"success": true}
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    try:
        data = request.get_json()

        analyzer = AdAnalyzer(user_id)

        # ì†Œìœ ê¶Œ í™•ì¸
        if not analyzer.check_ownership(snapshot_id):
            return create_error_response("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤", 403)

        success = analyzer.update_snapshot(snapshot_id, data)

        if success:
            return jsonify(create_success_response(message="ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤"))
        else:
            return create_error_response("ìˆ˜ì • ì‹¤íŒ¨", 500)

    except Exception as e:
        logger.error(f"Update snapshot failed: {e}")
        return create_error_response("ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", 500)


@ad_bp.route('/api/ad-analysis/snapshots/<int:snapshot_id>', methods=['DELETE'])
def delete_snapshot(snapshot_id):
    """
    ë¶„ì„ ì‚­ì œ

    Response:
        {"success": true}
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    try:
        analyzer = AdAnalyzer(user_id)

        # ì†Œìœ ê¶Œ í™•ì¸
        if not analyzer.check_ownership(snapshot_id):
            return create_error_response("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤", 403)

        success = analyzer.delete_snapshot(snapshot_id)

        if success:
            return jsonify(create_success_response(message="ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"))
        else:
            return create_error_response("ì‚­ì œ ì‹¤íŒ¨", 404)

    except Exception as e:
        logger.error(f"Delete snapshot failed: {e}")
        return create_error_response("ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", 500)


# ========================================
# 4. ë¹„êµ ë¶„ì„ API
# ========================================

@ad_bp.route('/api/ad-analysis/compare')
def compare_periods():
    """
    ê¸°ê°„ ë¹„êµ ë¶„ì„

    Query Params:
        - snapshot_a: ê¸°ì¤€ ë¶„ì„ ID
        - snapshot_b: ë¹„êµ ë¶„ì„ ID

    Response:
        {
            "comparison": {...},
            "summary": "..."
        }
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    snapshot_a = request.args.get('snapshot_a', type=int)
    snapshot_b = request.args.get('snapshot_b', type=int)

    if not snapshot_a or not snapshot_b:
        return create_error_response("ë‘ ê°œì˜ ìŠ¤ëƒ…ìƒ· IDê°€ í•„ìš”í•©ë‹ˆë‹¤", 400)

    try:
        analyzer = AdAnalyzer(user_id)

        # ì†Œìœ ê¶Œ í™•ì¸
        if not analyzer.check_ownership(snapshot_a) or not analyzer.check_ownership(snapshot_b):
            return create_error_response("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤", 403)

        comparison = analyzer.compare_snapshots(snapshot_a, snapshot_b)

        return jsonify(comparison)

    except Exception as e:
        logger.error(f"Compare snapshots failed: {e}")
        return create_error_response("ë¹„êµ ë¶„ì„ ì‹¤íŒ¨", 500)


# ========================================
# 5. ëª©í‘œ ê´€ë¦¬ API
# ========================================

@ad_bp.route('/api/ad-analysis/goals', methods=['GET', 'POST'])
def manage_goals():
    """
    ì›”ë³„ ëª©í‘œ ì„¤ì •/ì¡°íšŒ

    GET - Query Params:
        - year_month: YYYY-MM

    POST - Request Body:
        {
            "year_month": "2024-11",
            "budget": 10000000,
            "target_roas": 4.0,
            "target_revenue": 40000000
        }

    Response:
        {"goal": {...}}  (GET)
        {"success": true}  (POST)
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    if request.method == 'GET':
        year_month = request.args.get('year_month')

        if not year_month:
            return create_error_response("year_month íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤", 400)

        try:
            sql = """
                SELECT * FROM ad_monthly_goals
                WHERE user_id = %s AND year_month = %s
            """
            goal = execute_query(sql, (user_id, year_month), fetch_one=True)

            return jsonify({'goal': goal})

        except Exception as e:
            logger.error(f"Get goal failed: {e}")
            return create_error_response("ëª©í‘œ ì¡°íšŒ ì‹¤íŒ¨", 500)

    else:  # POST
        try:
            data = request.get_json()

            year_month = data.get('year_month')
            budget = data.get('budget')
            target_roas = data.get('target_roas')
            target_revenue = data.get('target_revenue')

            if not year_month:
                return create_error_response("year_monthê°€ í•„ìš”í•©ë‹ˆë‹¤", 400)

            # UPSERT (ON DUPLICATE KEY UPDATE)
            sql = """
                INSERT INTO ad_monthly_goals
                (user_id, year_month, budget, target_roas, target_revenue)
                VALUES (%s, %s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    budget = VALUES(budget),
                    target_roas = VALUES(target_roas),
                    target_revenue = VALUES(target_revenue)
            """

            execute_insert(sql, (user_id, year_month, budget, target_roas, target_revenue))

            return jsonify(create_success_response(message="ëª©í‘œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"))

        except Exception as e:
            logger.error(f"Save goal failed: {e}")
            return create_error_response("ëª©í‘œ ì €ì¥ ì‹¤íŒ¨", 500)


@ad_bp.route('/api/ad-analysis/budget-pacing')
def budget_pacing():
    """
    ì˜ˆì‚° ì†Œì§„ìœ¨ ë° í˜ì´ì‹± ë¶„ì„

    Query Params:
        - year_month: YYYY-MM

    Response:
        {
            "budget": 10000000,
            "spent": 5800000,
            "spent_rate": 58,
            "status": "FAST",
            ...
        }
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id
    year_month = request.args.get('year_month')

    if not year_month:
        return create_error_response("year_month íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤", 400)

    try:
        analyzer = AdAnalyzer(user_id)
        pacing = analyzer.calculate_budget_pacing(year_month)

        return jsonify(pacing)

    except Exception as e:
        logger.error(f"Budget pacing failed: {e}")
        return create_error_response("ì˜ˆì‚° ë¶„ì„ ì‹¤íŒ¨", 500)


# ========================================
# 6. ìº í˜ì¸ ë©”ëª¨ API
# ========================================

@ad_bp.route('/api/ad-analysis/memos', methods=['GET', 'POST'])
def manage_memos():
    """
    ìº í˜ì¸ ë©”ëª¨ ì¡°íšŒ/ì¶”ê°€

    GET - Query Params:
        - campaign_name: ìº í˜ì¸ëª…

    POST - Request Body:
        {
            "campaign_name": "ë¸”í”„_ì‹ ê·œ",
            "memo": "ì†Œì¬ #3ìœ¼ë¡œ êµì²´"
        }

    Response:
        {"memos": [...]}  (GET)
        {"success": true}  (POST)
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    if request.method == 'GET':
        campaign_name = request.args.get('campaign_name')

        try:
            sql = """
                SELECT * FROM ad_campaign_memos
                WHERE user_id = %s
            """
            params = [user_id]

            if campaign_name:
                sql += " AND campaign_name = %s"
                params.append(campaign_name)

            sql += " ORDER BY created_at DESC"

            memos = execute_query(sql, tuple(params))

            # ë‚ ì§œ í¬ë§·íŒ…
            for memo in memos:
                memo['created_at'] = memo['created_at'].strftime('%Y-%m-%d %H:%M:%S')

            return jsonify({'memos': memos})

        except Exception as e:
            logger.error(f"Get memos failed: {e}")
            return create_error_response("ë©”ëª¨ ì¡°íšŒ ì‹¤íŒ¨", 500)

    else:  # POST
        try:
            data = request.get_json()

            campaign_name = data.get('campaign_name')
            memo = data.get('memo')

            if not campaign_name or not memo:
                return create_error_response("campaign_nameê³¼ memoê°€ í•„ìš”í•©ë‹ˆë‹¤", 400)

            sql = """
                INSERT INTO ad_campaign_memos
                (user_id, campaign_name, memo)
                VALUES (%s, %s, %s)
            """

            execute_insert(sql, (user_id, campaign_name, memo))

            return jsonify(create_success_response(message="ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"))

        except Exception as e:
            logger.error(f"Save memo failed: {e}")
            return create_error_response("ë©”ëª¨ ì €ì¥ ì‹¤íŒ¨", 500)


# ========================================
# 7. ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸° API
# ========================================

@ad_bp.route('/api/ad-analysis/export/pdf/<int:snapshot_id>')
def export_pdf(snapshot_id):
    """
    PDF ë¦¬í¬íŠ¸ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    try:
        analyzer = AdAnalyzer(user_id)

        if not analyzer.check_ownership(snapshot_id):
            return create_error_response("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤", 403)

        # TODO: PDF ìƒì„± ë¡œì§ êµ¬í˜„
        return create_error_response("PDF ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤", 501)

    except Exception as e:
        logger.error(f"Export PDF failed: {e}")
        return create_error_response("PDF ìƒì„± ì‹¤íŒ¨", 500)


@ad_bp.route('/api/ad-analysis/export/excel/<int:snapshot_id>')
def export_excel(snapshot_id):
    """
    Excel ë¦¬í¬íŠ¸ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
    """
    user_id = get_current_user_id()  # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ user_id

    try:
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils.dataframe import dataframe_to_rows
        from datetime import datetime
        import tempfile

        analyzer = AdAnalyzer(user_id)

        if not analyzer.check_ownership(snapshot_id):
            return create_error_response("ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤", 403)

        # ë°ì´í„° ì¡°íšŒ
        snapshot_data = analyzer.get_snapshot_detail(snapshot_id)
        if not snapshot_data:
            return create_error_response("ë¶„ì„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", 404)

        metrics = snapshot_data.get('metrics', {})
        snapshot_info = snapshot_data.get('snapshot', {})

        # Excel ì›Œí¬ë¶ ìƒì„±
        wb = Workbook()

        # ìŠ¤íƒ€ì¼ ì •ì˜
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(color="FFFFFF", bold=True, size=12)
        header_alignment = Alignment(horizontal="center", vertical="center")
        border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )

        # Sheet 1: ìš”ì•½
        ws_summary = wb.active
        ws_summary.title = "ìš”ì•½"

        # ì œëª©
        ws_summary['A1'] = "ê´‘ê³  ë¶„ì„ ë¦¬í¬íŠ¸"
        ws_summary['A1'].font = Font(size=16, bold=True)
        ws_summary.merge_cells('A1:D1')

        # ê¸°ë³¸ ì •ë³´
        ws_summary['A3'] = "ë¶„ì„ëª…"
        ws_summary['B3'] = snapshot_info.get('snapshot_name', 'N/A')
        ws_summary['A4'] = "ë¶„ì„ ê¸°ê°„"
        ws_summary['B4'] = f"{snapshot_info.get('period_start', 'N/A')} ~ {snapshot_info.get('period_end', 'N/A')}"
        ws_summary['A5'] = "ìƒì„±ì¼ì‹œ"
        ws_summary['B5'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # ì£¼ìš” ì§€í‘œ
        ws_summary['A7'] = "ì£¼ìš” ì§€í‘œ"
        ws_summary['A7'].font = Font(size=14, bold=True)

        summary_data = [
            ["ì§€í‘œ", "ê°’"],
            ["ì´ ì§€ì¶œ", f"{metrics.get('total_spend', 0):,.0f}ì›"],
            ["ì´ ë§¤ì¶œ", f"{metrics.get('total_revenue', 0):,.0f}ì›"],
            ["í‰ê·  ROAS", f"{metrics.get('avg_roas', 0):.2f}"],
            ["í‰ê·  CTR", f"{metrics.get('avg_ctr', 0):.2f}%"],
            ["í‰ê·  CPC", f"{metrics.get('avg_cpc', 0):,.0f}ì›"],
            ["í‰ê·  CPA", f"{metrics.get('avg_cpa', 0):,.0f}ì›"],
            ["ì „í™˜ìœ¨", f"{metrics.get('cvr', 0):.2f}%"],
            ["ì´ í´ë¦­", f"{metrics.get('total_clicks', 0):,}"],
            ["ì´ ì „í™˜", f"{metrics.get('total_conversions', 0):,}"],
        ]

        for row_idx, row_data in enumerate(summary_data, start=8):
            for col_idx, value in enumerate(row_data, start=1):
                cell = ws_summary.cell(row=row_idx, column=col_idx, value=value)
                if row_idx == 8:  # í—¤ë”
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = header_alignment
                cell.border = border

        # ì—´ ë„ˆë¹„ ì¡°ì •
        ws_summary.column_dimensions['A'].width = 20
        ws_summary.column_dimensions['B'].width = 25

        # Sheet 2: ìº í˜ì¸ ì„±ê³¼
        ws_campaigns = wb.create_sheet("ìº í˜ì¸ ì„±ê³¼")
        campaigns = metrics.get('campaigns', [])

        if campaigns:
            campaign_headers = ["ìˆœìœ„", "ìº í˜ì¸ëª…", "ê´‘ê³ ìœ í˜•", "ROAS", "CTR(%)", "CPA(ì›)", "CVR(%)", "ì§€ì¶œ(ì›)", "ë§¤ì¶œ(ì›)", "í´ë¦­", "ì „í™˜"]

            # í—¤ë” ì‘ì„±
            for col_idx, header in enumerate(campaign_headers, start=1):
                cell = ws_campaigns.cell(row=1, column=col_idx, value=header)
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = header_alignment
                cell.border = border

            # ë°ì´í„° ì‘ì„±
            for row_idx, campaign in enumerate(campaigns, start=2):
                row_data = [
                    campaign.get('rank', row_idx - 1),
                    campaign.get('campaign_name', 'N/A'),
                    "ë§¤ì¶œí˜•" if campaign.get('ad_type') == 'sales' else "ì ì¬ê³ ê°",
                    round(campaign.get('roas', 0), 2),
                    round(campaign.get('ctr', 0), 2),
                    int(campaign.get('cpa', 0)),
                    round(campaign.get('cvr', 0), 2),
                    int(campaign.get('spend', 0)),
                    int(campaign.get('revenue', 0)),
                    int(campaign.get('clicks', 0)),
                    int(campaign.get('conversions', 0))
                ]

                for col_idx, value in enumerate(row_data, start=1):
                    cell = ws_campaigns.cell(row=row_idx, column=col_idx, value=value)
                    cell.border = border

                    # ìˆ«ì ì„œì‹
                    if col_idx in [4, 5, 7]:  # ROAS, CTR, CVR
                        cell.number_format = '0.00'
                    elif col_idx in [6, 8, 9]:  # CPA, ì§€ì¶œ, ë§¤ì¶œ
                        cell.number_format = '#,##0'

            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
            for col in ws_campaigns.columns:
                max_length = 0
                column = col[0].column_letter
                for cell in col:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_campaigns.column_dimensions[column].width = adjusted_width

        # Sheet 3: ì¼ë³„ ë°ì´í„°
        ws_daily = wb.create_sheet("ì¼ë³„ ë°ì´í„°")
        daily_data = metrics.get('daily_data', metrics.get('daily_trend', []))

        if daily_data:
            # í—¤ë” ê²°ì • (campaign_name í¬í•¨ ì—¬ë¶€)
            sample_row = daily_data[0] if daily_data else {}
            has_campaign = 'campaign_name' in sample_row

            if has_campaign:
                daily_headers = ["ë‚ ì§œ", "ìº í˜ì¸ëª…", "ì§€ì¶œ(ì›)", "ë§¤ì¶œ(ì›)", "ROAS", "í´ë¦­", "ì „í™˜", "CTR(%)", "CVR(%)"]
            else:
                daily_headers = ["ë‚ ì§œ", "ì§€ì¶œ(ì›)", "ë§¤ì¶œ(ì›)", "ROAS", "í´ë¦­", "ì „í™˜", "CTR(%)", "CVR(%)"]

            # í—¤ë” ì‘ì„±
            for col_idx, header in enumerate(daily_headers, start=1):
                cell = ws_daily.cell(row=1, column=col_idx, value=header)
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = header_alignment
                cell.border = border

            # ë°ì´í„° ì‘ì„±
            for row_idx, daily in enumerate(daily_data, start=2):
                if has_campaign:
                    row_data = [
                        daily.get('date', 'N/A'),
                        daily.get('campaign_name', 'N/A'),
                        int(daily.get('spend', 0)),
                        int(daily.get('revenue', 0)),
                        round(daily.get('roas', 0), 2),
                        int(daily.get('clicks', 0)),
                        int(daily.get('conversions', 0)),
                        round(daily.get('ctr', 0), 2),
                        round(daily.get('cvr', 0), 2)
                    ]
                else:
                    row_data = [
                        daily.get('date', 'N/A'),
                        int(daily.get('spend', 0)),
                        int(daily.get('revenue', 0)),
                        round(daily.get('roas', 0), 2),
                        int(daily.get('clicks', 0)),
                        int(daily.get('conversions', 0)),
                        round(daily.get('ctr', 0), 2),
                        round(daily.get('cvr', 0), 2)
                    ]

                for col_idx, value in enumerate(row_data, start=1):
                    cell = ws_daily.cell(row=row_idx, column=col_idx, value=value)
                    cell.border = border

            # ì—´ ë„ˆë¹„ ì¡°ì •
            for col in ws_daily.columns:
                max_length = 0
                column = col[0].column_letter
                for cell in col:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_daily.column_dimensions[column].width = adjusted_width

        # Sheet 4: ì†Œì¬ ì„±ê³¼ (ìˆëŠ” ê²½ìš°)
        creatives = metrics.get('creatives', [])
        if creatives:
            ws_creatives = wb.create_sheet("ì†Œì¬ ì„±ê³¼")

            creative_headers = ["ìˆœìœ„", "ì†Œì¬ëª…", "í”Œë«í¼", "ìœ í˜•", "ROAS", "CTR(%)", "CVR(%)", "ì§€ì¶œ(ì›)", "ë§¤ì¶œ(ì›)", "í´ë¦­", "ì „í™˜"]

            # í—¤ë”
            for col_idx, header in enumerate(creative_headers, start=1):
                cell = ws_creatives.cell(row=1, column=col_idx, value=header)
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = header_alignment
                cell.border = border

            # ë°ì´í„°
            for row_idx, creative in enumerate(creatives, start=2):
                row_data = [
                    creative.get('roas_rank', row_idx - 1),
                    creative.get('ad_creative_name', 'N/A'),
                    creative.get('platform', 'N/A'),
                    creative.get('creative_type', 'N/A'),
                    round(creative.get('roas', 0), 2),
                    round(creative.get('ctr', 0), 2),
                    round(creative.get('cvr', 0), 2),
                    int(creative.get('spend', 0)),
                    int(creative.get('revenue', 0)),
                    int(creative.get('clicks', 0)),
                    int(creative.get('conversions', 0))
                ]

                for col_idx, value in enumerate(row_data, start=1):
                    cell = ws_creatives.cell(row=row_idx, column=col_idx, value=value)
                    cell.border = border

            # ì—´ ë„ˆë¹„ ì¡°ì •
            for col in ws_creatives.columns:
                max_length = 0
                column = col[0].column_letter
                for cell in col:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                ws_creatives.column_dimensions[column].width = adjusted_width

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_dir = tempfile.gettempdir()
        filename = f"ad_report_{snapshot_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        filepath = os.path.join(temp_dir, filename)

        wb.save(filepath)

        logger.info(f"Excel file created: {filepath}")

        # íŒŒì¼ ì „ì†¡
        return send_file(
            filepath,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except ImportError:
        logger.error("openpyxl not installed")
        return create_error_response("Excel ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openpyxlì„ ì‹¤í–‰í•˜ì„¸ìš”.", 500)
    except Exception as e:
        logger.error(f"Export Excel failed: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"Excel ìƒì„± ì‹¤íŒ¨: {str(e)}", 500)


# ========================================
# 8. í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ API
# ========================================

@ad_bp.route('/api/ad-analysis/template/<template_type>')
def download_template(template_type):
    """
    Excel í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ

    template_type: 'unified', 'naver', 'meta', 'google', 'kakao', 'generic'
    """
    templates = {
        'unified': 'ad_template_unified.xlsx',  # í†µí•© í…œí”Œë¦¿ (í•œê¸€ ì»¬ëŸ¼, ê´‘ê³ ìœ í˜• í¬í•¨)
        'generic': 'ad_template_generic.xlsx',
        'naver': 'ad_template_naver.xlsx',
        'meta': 'ad_template_meta.xlsx',
        'google': 'ad_template_google.xlsx',
        'kakao': 'ad_template_kakao.xlsx'
    }

    filename = templates.get(template_type, templates['generic'])
    template_path = os.path.join(current_app.root_path, 'static', 'templates', filename)

    if not os.path.exists(template_path):
        return create_error_response("í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", 404)

    return send_file(template_path, as_attachment=True, download_name=filename)


# ========================================
# Helper Functions
# ========================================

def _calculate_creative_metrics(df):
    """
    ì†Œì¬ë³„ ì„±ê³¼ ì§€í‘œ ê³„ì‚°

    Args:
        df: pandas DataFrame with columns including ad_creative_name

    Returns:
        list: ì†Œì¬ë³„ ì§€í‘œ ë¦¬ìŠ¤íŠ¸ (ROAS ìˆœìœ„ìˆœ)
    """
    import numpy as np

    # ad_creative_nameìœ¼ë¡œ ê·¸ë£¹í™”
    creative_stats = df.groupby('ad_creative_name').agg({
        'spend': 'sum',
        'revenue': 'sum',
        'clicks': 'sum',
        'conversions': 'sum',
        'impressions': 'sum' if 'impressions' in df.columns else lambda x: 0
    }).reset_index()

    # ROAS ê³„ì‚°
    creative_stats['roas'] = (creative_stats['revenue'] / creative_stats['spend']).round(2)

    # CTR ê³„ì‚°
    if 'impressions' in df.columns and creative_stats['impressions'].sum() > 0:
        creative_stats['ctr'] = (creative_stats['clicks'] / creative_stats['impressions'] * 100).round(2)
    else:
        creative_stats['ctr'] = 0

    # CPA ê³„ì‚°
    creative_stats['cpa'] = (creative_stats['spend'] / creative_stats['conversions']).replace([np.inf, -np.inf], 0).round(0)

    # CVR (ì „í™˜ìœ¨) ê³„ì‚°
    creative_stats['cvr'] = (creative_stats['conversions'] / creative_stats['clicks'] * 100).replace([np.inf, -np.inf], 0).round(2)

    # CPC ê³„ì‚°
    creative_stats['cpc'] = (creative_stats['spend'] / creative_stats['clicks']).replace([np.inf, -np.inf], 0).round(0)

    # ê°ë‹¨ê°€ ê³„ì‚°
    creative_stats['avg_order_value'] = (creative_stats['revenue'] / creative_stats['conversions']).replace([np.inf, -np.inf], 0).round(0)

    # ROAS ìˆœìœ„ ê³„ì‚°
    creative_stats = creative_stats.sort_values('roas', ascending=False)
    creative_stats['roas_rank'] = range(1, len(creative_stats) + 1)

    # CVR ìˆœìœ„ ê³„ì‚° (ë³„ë„ ì •ë ¬)
    creative_stats_cvr_sorted = creative_stats.sort_values('cvr', ascending=False)
    creative_stats['cvr_rank'] = creative_stats_cvr_sorted.index.map(lambda x: creative_stats_cvr_sorted.index.get_loc(x) + 1)

    # ì†Œì¬ íƒ€ì… ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if 'ad_creative_type' in df.columns:
        creative_type_map = df.groupby('ad_creative_name')['ad_creative_type'].first()
        creative_stats['creative_type'] = creative_stats['ad_creative_name'].map(creative_type_map)

    # í”Œë«í¼ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if 'platform' in df.columns:
        platform_map = df.groupby('ad_creative_name')['platform'].first()
        creative_stats['platform'] = creative_stats['ad_creative_name'].map(platform_map)

    # ìƒíƒœ íŒì • (ROAS ê¸°ì¤€)
    def get_creative_status(roas):
        if roas >= 4.0:
            return 'excellent'
        elif roas >= 3.0:
            return 'good'
        else:
            return 'poor'

    creative_stats['status'] = creative_stats['roas'].apply(get_creative_status)

    # NaNì„ 0ìœ¼ë¡œ ë³€í™˜
    creative_stats = creative_stats.fillna(0)

    return creative_stats.to_dict('records')


def _calculate_metrics_inmemory(df):
    """
    In-Memory ë°©ì‹ìœ¼ë¡œ ì§€í‘œ ê³„ì‚° (ë°ì´í„°ë² ì´ìŠ¤ ì—†ì´)

    Args:
        df: pandas DataFrame with columns: date, campaign_name, spend, clicks, conversions, revenue, impressions (optional)

    Returns:
        dict: ê³„ì‚°ëœ ë©”íŠ¸ë¦­ìŠ¤
    """
    import numpy as np

    # ì „ì²´ ì§€í‘œ ê³„ì‚°
    total_spend = df['spend'].sum()
    total_revenue = df['revenue'].sum()
    total_clicks = df['clicks'].sum()
    total_conversions = df['conversions'].sum()
    total_impressions = df['impressions'].sum() if 'impressions' in df.columns else 0

    # ê³„ì‚° ì§€í‘œ
    avg_roas = round(total_revenue / total_spend, 2) if total_spend > 0 else 0
    avg_ctr = round((total_clicks / total_impressions * 100), 2) if total_impressions > 0 else 0
    avg_cpc = round(total_spend / total_clicks, 0) if total_clicks > 0 else 0
    avg_cpa = round(total_spend / total_conversions, 0) if total_conversions > 0 else 0
    cvr = round((total_conversions / total_clicks * 100), 2) if total_clicks > 0 else 0
    avg_order_value = round(total_revenue / total_conversions, 0) if total_conversions > 0 else 0

    # ìº í˜ì¸ë³„ í†µê³„ (ê´‘ê³ ìœ í˜• í¬í•¨)
    agg_dict = {
        'spend': 'sum',
        'revenue': 'sum',
        'clicks': 'sum',
        'conversions': 'sum',
        'impressions': 'sum' if 'impressions' in df.columns else lambda x: 0
    }

    # ad_typeì´ ìˆìœ¼ë©´ ê·¸ë£¹í™”ì— í¬í•¨
    if 'ad_type' in df.columns:
        # ìº í˜ì¸ë³„ ad_typeì€ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ë™ì¼ ìº í˜ì¸ì€ ë™ì¼ ìœ í˜•ì´ë¼ê³  ê°€ì •)
        campaign_stats = df.groupby('campaign_name').agg({
            **agg_dict,
            'ad_type': 'first'
        }).reset_index()
    else:
        campaign_stats = df.groupby('campaign_name').agg(agg_dict).reset_index()
        campaign_stats['ad_type'] = 'sales'  # ê¸°ë³¸ê°’

    # ê³µí†µ ì§€í‘œ ê³„ì‚°
    campaign_stats['roas'] = (campaign_stats['revenue'] / campaign_stats['spend']).replace([np.inf, -np.inf], 0).round(2)
    campaign_stats['cpl'] = (campaign_stats['spend'] / campaign_stats['conversions']).replace([np.inf, -np.inf], 0).round(0)
    campaign_stats['cpa'] = campaign_stats['cpl']  # CPA = CPL (ê°™ì€ ê³„ì‚°)

    if 'impressions' in df.columns and campaign_stats['impressions'].sum() > 0:
        campaign_stats['ctr'] = (campaign_stats['clicks'] / campaign_stats['impressions'] * 100).round(2)
    else:
        campaign_stats['ctr'] = 0

    campaign_stats['cvr'] = (campaign_stats['conversions'] / campaign_stats['clicks'] * 100).replace([np.inf, -np.inf], 0).round(2)

    # ê´‘ê³ ìœ í˜•ë³„ ì£¼ìš”ì§€í‘œ ë° ì •ë ¬
    def get_primary_metric(row):
        if row['ad_type'] == 'lead':
            return 'CPL'
        else:
            return 'ROAS'

    def get_primary_value(row):
        if row['ad_type'] == 'lead':
            return row['cpl']
        else:
            return row['roas']

    campaign_stats['primary_metric'] = campaign_stats.apply(get_primary_metric, axis=1)
    campaign_stats['primary_value'] = campaign_stats.apply(get_primary_value, axis=1)

    # ê´‘ê³ ìœ í˜•ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì •ë ¬ í›„ í•©ì¹˜ê¸°
    sales_campaigns = campaign_stats[campaign_stats['ad_type'] == 'sales'].sort_values('roas', ascending=False)
    lead_campaigns = campaign_stats[campaign_stats['ad_type'] == 'lead'].sort_values('cpl', ascending=True)

    # ë§¤ì¶œí˜• ë¨¼ì €, ì ì¬ê³ ê° ë‚˜ì¤‘ì— (ê°ê° ìˆœìœ„ ë¶€ì—¬)
    sales_campaigns['rank'] = range(1, len(sales_campaigns) + 1)
    lead_campaigns['rank'] = range(1, len(lead_campaigns) + 1)

    campaign_stats = pd.concat([sales_campaigns, lead_campaigns], ignore_index=True)

    # ìƒíƒœ íŒì • ì œê±° (ìˆœìœ„ë¡œ ëŒ€ì²´)
    campaign_stats['status'] = 'normal'  # ìƒíƒœ ë°°ì§€ ì‚¬ìš© ì•ˆ í•¨

    # ì¼ë³„ íŠ¸ë Œë“œ - ë‚ ì§œë³„ ì „ì²´ í•©ê³„ë¡œ ì§‘ê³„ (ì°¨íŠ¸ í‘œì‹œìš©)
    daily = df.groupby('date').agg({
        'spend': 'sum',
        'revenue': 'sum',
        'clicks': 'sum',
        'conversions': 'sum',
        'impressions': 'sum' if 'impressions' in df.columns else lambda x: 0
    }).reset_index()

    daily['roas'] = (daily['revenue'] / daily['spend']).replace([np.inf, -np.inf], 0).round(2)

    if 'impressions' in df.columns:
        daily['ctr'] = (daily['clicks'] / daily['impressions'] * 100).replace([np.inf, -np.inf], 0).round(2)
    else:
        daily['ctr'] = 0

    daily['cvr'] = (daily['conversions'] / daily['clicks'] * 100).replace([np.inf, -np.inf], 0).round(2)

    # NaNì„ 0ìœ¼ë¡œ ë³€í™˜
    daily = daily.fillna(0)
    campaign_stats = campaign_stats.fillna(0)

    # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (Chart.js í˜¸í™˜ì„±)
    daily['date'] = daily['date'].astype(str)

    # campaign_nameì„ ë³´ì¡´í•œ ì›ë³¸ ë°ì´í„° ìƒì„± (ìº í˜ì¸ ë¶„ì„ìš©)
    daily_data_columns = ['date', 'campaign_name', 'spend', 'revenue', 'clicks', 'conversions']
    if 'impressions' in df.columns:
        daily_data_columns.append('impressions')
    if 'ad_type' in df.columns:
        daily_data_columns.append('ad_type')

    daily_data = df[daily_data_columns].copy()
    daily_data['date'] = daily_data['date'].astype(str)
    daily_data = daily_data.fillna(0)

    # ì†Œì¬ë³„ ë¶„ì„ (ad_creative_name ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
    creatives = []
    if 'ad_creative_name' in df.columns:
        creatives = _calculate_creative_metrics(df)

    metrics = {
        # ê¸°ë³¸ ì§€í‘œ
        'total_spend': float(total_spend),
        'total_revenue': float(total_revenue),
        'total_clicks': int(total_clicks),
        'total_conversions': int(total_conversions),
        'total_impressions': int(total_impressions),

        # ê³„ì‚° ì§€í‘œ
        'avg_roas': avg_roas,
        'avg_ctr': avg_ctr,
        'avg_cpc': avg_cpc,
        'avg_cpa': avg_cpa,
        'cvr': cvr,
        'avg_order_value': avg_order_value,

        # ìº í˜ì¸ë³„ í†µê³„
        'campaigns': campaign_stats.to_dict('records'),

        # ì¼ë³„ íŠ¸ë Œë“œ (ì°¨íŠ¸ìš© - ë‚ ì§œë³„ ì§‘ê³„)
        'daily_trend': daily.to_dict('records'),

        # ì¼ë³„ ìƒì„¸ ë°ì´í„° (ìº í˜ì¸ ë¶„ì„ìš© - campaign_name í¬í•¨)
        'daily_data': daily_data.to_dict('records'),

        # ì†Œì¬ë³„ í†µê³„ (ìˆëŠ” ê²½ìš°ë§Œ)
        'creatives': creatives,
    }

    return metrics
