"""
ì¿ íŒ¡ ê´‘ê³  ë¶„ì„ ì „ìš© ë¼ìš°íŠ¸
- ì¿ íŒ¡ ëŒ€ì‹œë³´ë“œ
- ì¿ íŒ¡ íŒŒì¼ ì—…ë¡œë“œ
- ì¿ íŒ¡ í‚¤ì›Œë“œ ì¶”ì²œ
"""

import os
import pandas as pd
import numpy as np
import logging
from flask import (
    Blueprint, render_template, request, jsonify,
    session, current_app
)

logger = logging.getLogger(__name__)

# Blueprint ìƒì„±
coupang_bp = Blueprint('ad_coupang', __name__)


# ========================================
# ì¿ íŒ¡ ëŒ€ì‹œë³´ë“œ
# ========================================

@coupang_bp.route('/ad-dashboard/coupang-test')
def coupang_manual_test():
    """
    ì¿ íŒ¡ ê´‘ê³  ìˆ˜ë™ í…ŒìŠ¤íŠ¸ í˜ì´ì§€

    Returns:
        HTML: ìˆ˜ë™ í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿
    """
    return render_template('manual_test.html')


@coupang_bp.route('/ad-dashboard/coupang')
@coupang_bp.route('/ad-dashboard-coupang')  # ë³„ì¹­ ë¼ìš°íŠ¸
# @require_auth  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¸ì¦ ë¹„í™œì„±í™”
def coupang_dashboard():
    """
    ì¿ íŒ¡ ê´‘ê³  ì „ìš© ëŒ€ì‹œë³´ë“œ

    Returns:
        HTML: ì¿ íŒ¡ ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿
    """
    # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ ì‚¬ìš©ì ì •ë³´
    user = {
        'user_id': 'test_user',
        'username': 'í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì',
        'email': 'test@example.com'
    }

    return render_template('ad_dashboard_coupang.html', user=user)


# ========================================
# ì¿ íŒ¡ íŒŒì¼ ì—…ë¡œë“œ
# ========================================

@coupang_bp.route('/api/ad-analysis/upload-coupang', methods=['POST'])
# @require_auth  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¸ì¦ ë¹„í™œì„±í™”
def upload_coupang():
    """
    ì¿ íŒ¡ ê´‘ê³  Excel íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹±

    ì¿ íŒ¡ ê´‘ê³  ë³´ê³ ì„œ í•„ìˆ˜ ì»¬ëŸ¼:
    - í‚¤ì›Œë“œ, ë…¸ì¶œìˆ˜, í´ë¦­ìˆ˜, ê´‘ê³ ë¹„, í´ë¦­ë¥ 
    - ì´ ì£¼ë¬¸ìˆ˜(1ì¼), ì´ íŒë§¤ìˆ˜ëŸ‰(1ì¼), ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼)
    - ì´ê´‘ê³ ìˆ˜ìµë¥ (1ì¼) = ROAS
    """
    user_id = 'test_user'

    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'}), 400

    file = request.files['file']

    if file.filename == '':
        return jsonify({'success': False, 'error': 'íŒŒì¼ëª…ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤'}), 400

    try:
        # Excel íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ë¬¸ì œ í•´ê²° - BytesIO ì‚¬ìš©)
        import io
        file_content = file.read()
        df = pd.read_excel(io.BytesIO(file_content), engine='openpyxl')
        logger.info(f'Coupang file uploaded: {file.filename}, rows: {len(df)}, columns: {len(df.columns)}')

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (ë§¤ì¶œì•¡ì€ 14ì¼ ìš°ì„ , ì—†ìœ¼ë©´ 1ì¼ ì‚¬ìš©)
        required_cols_base = ['í‚¤ì›Œë“œ', 'ë…¸ì¶œìˆ˜', 'í´ë¦­ìˆ˜', 'ê´‘ê³ ë¹„', 'í´ë¦­ë¥ ']

        # ë§¤ì¶œì•¡ ì»¬ëŸ¼ ì„ íƒ: 14ì¼ ìš°ì„ , ì—†ìœ¼ë©´ 1ì¼ ì‚¬ìš©
        if 'ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼)' in df.columns:
            revenue_col = 'ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼)'
            logger.info('Using 14-day revenue data')
        elif 'ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼)' in df.columns:
            revenue_col = 'ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼)'
            logger.info('Using 1-day revenue data (14-day not available)')
        else:
            logger.error('No revenue column found')
            return jsonify({'success': False, 'error': 'ë§¤ì¶œì•¡ ì»¬ëŸ¼ ì—†ìŒ (ì´ ì „í™˜ë§¤ì¶œì•¡(14ì¼) ë˜ëŠ” ì´ ì „í™˜ë§¤ì¶œì•¡(1ì¼) í•„ìš”)'}), 400

        # ROAS ì»¬ëŸ¼ ì„ íƒ
        if 'ì´ê´‘ê³ ìˆ˜ìµë¥ (14ì¼)' in df.columns:
            roas_col = 'ì´ê´‘ê³ ìˆ˜ìµë¥ (14ì¼)'
        elif 'ì´ê´‘ê³ ìˆ˜ìµë¥ (1ì¼)' in df.columns:
            roas_col = 'ì´ê´‘ê³ ìˆ˜ìµë¥ (1ì¼)'
        else:
            roas_col = None

        # ì£¼ë¬¸ìˆ˜/íŒë§¤ìˆ˜ëŸ‰ ì»¬ëŸ¼ ì„ íƒ
        if 'ì´ ì£¼ë¬¸ìˆ˜(14ì¼)' in df.columns:
            order_col = 'ì´ ì£¼ë¬¸ìˆ˜(14ì¼)'
            quantity_col = 'ì´ íŒë§¤ìˆ˜ëŸ‰(14ì¼)' if 'ì´ íŒë§¤ìˆ˜ëŸ‰(14ì¼)' in df.columns else 'ì´ íŒë§¤ìˆ˜ëŸ‰(1ì¼)'
        else:
            order_col = 'ì´ ì£¼ë¬¸ìˆ˜(1ì¼)'
            quantity_col = 'ì´ íŒë§¤ìˆ˜ëŸ‰(1ì¼)'

        required_cols = required_cols_base + [order_col, quantity_col, revenue_col]
        if roas_col:
            required_cols.append(roas_col)

        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            logger.error(f'Missing required columns: {missing}')
            return jsonify({'success': False, 'error': f'í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}'}), 400

        # ì»¬ëŸ¼ëª… í†µì¼ (14ì¼/1ì¼ ìƒê´€ì—†ì´ ë™ì¼í•œ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©)
        df = df.rename(columns={
            revenue_col: 'ì´ ì „í™˜ë§¤ì¶œì•¡',
            order_col: 'ì´ ì£¼ë¬¸ìˆ˜',
            quantity_col: 'ì´ íŒë§¤ìˆ˜ëŸ‰'
        })
        if roas_col:
            df = df.rename(columns={roas_col: 'ì´ê´‘ê³ ìˆ˜ìµë¥ '})

        logger.info(f'Column mapping: revenue={revenue_col}, orders={order_col}')

        # ë°ì´í„° ì •ì œ
        # 1. ëª¨ë“  ê´‘ê³  ë…¸ì¶œ ì§€ë©´ ë°ì´í„° í¬í•¨ (ê²€ìƒ‰ì˜ì—­ + ë¹„ê²€ìƒ‰ì˜ì—­ + ë¦¬íƒ€ê²ŸíŒ…)
        # í‚¤ì›Œë“œê°€ '-'ì—¬ë„ í¬í•¨ (ë¹„ê²€ìƒ‰ì˜ì—­, ë¦¬íƒ€ê²ŸíŒ…ì˜ í‚¤ì›Œë“œëŠ” '-'ì„)
        if 'ê´‘ê³  ë…¸ì¶œ ì§€ë©´' in df.columns:
            exposure_types = df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'].value_counts()
            logger.info(f'Ad exposure types included: {exposure_types.to_dict()}')

        # ğŸ”¥ ë¹„ê²€ìƒ‰ì˜ì—­ ë° ë¦¬íƒ€ê²ŸíŒ… í†µí•© ì²˜ë¦¬
        if 'ê´‘ê³  ë…¸ì¶œ ì§€ë©´' in df.columns:
            # 1) ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•©
            non_search_mask = df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'].str.contains('ë¹„ê²€ìƒ‰', na=False)
            # 2) ë¦¬íƒ€ê²ŸíŒ… í†µí•©
            retargeting_mask = df['ê´‘ê³  ë…¸ì¶œ ì§€ë©´'].str.contains('ë¦¬íƒ€ê²ŸíŒ…', na=False)

            # ê²€ìƒ‰ì˜ì—­ë§Œ ë‚¨ê¹€ (ë¹„ê²€ìƒ‰, ë¦¬íƒ€ê²ŸíŒ… ì œì™¸)
            search_only_df = df[~(non_search_mask | retargeting_mask)].copy()

            aggregated_rows = []

            # === ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•© ===
            if non_search_mask.sum() > 0:
                non_search_df = df[non_search_mask].copy()
                logger.info(f'ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•© ì „: {non_search_mask.sum()}ê°œ í–‰')

                # ë¹„ê²€ìƒ‰ì˜ì—­ ì§€í‘œ í•©ì‚°
                non_search_aggregated = {
                    'í‚¤ì›Œë“œ': 'ë¹„ê²€ìƒ‰ì˜ì—­ (í†µí•©)',
                    'ê´‘ê³  ë…¸ì¶œ ì§€ë©´': 'ë¹„ê²€ìƒ‰ì˜ì—­ (í†µí•©)',
                    'ë…¸ì¶œìˆ˜': non_search_df['ë…¸ì¶œìˆ˜'].sum(),
                    'í´ë¦­ìˆ˜': non_search_df['í´ë¦­ìˆ˜'].sum(),
                    'ê´‘ê³ ë¹„': non_search_df['ê´‘ê³ ë¹„'].sum(),
                    'ì´ ì£¼ë¬¸ìˆ˜': non_search_df['ì´ ì£¼ë¬¸ìˆ˜'].sum(),
                    'ì´ íŒë§¤ìˆ˜ëŸ‰': non_search_df['ì´ íŒë§¤ìˆ˜ëŸ‰'].sum(),
                    'ì´ ì „í™˜ë§¤ì¶œì•¡': non_search_df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()
                }

                # í´ë¦­ë¥  ì¬ê³„ì‚°
                if non_search_aggregated['ë…¸ì¶œìˆ˜'] > 0:
                    non_search_aggregated['í´ë¦­ë¥ '] = (non_search_aggregated['í´ë¦­ìˆ˜'] / non_search_aggregated['ë…¸ì¶œìˆ˜']) * 100
                else:
                    non_search_aggregated['í´ë¦­ë¥ '] = 0

                # ROAS ì¬ê³„ì‚° (ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ Excel ë°ì´í„°ì™€ ì¼ì¹˜)
                if non_search_aggregated['ê´‘ê³ ë¹„'] > 0:
                    roas_value = (non_search_aggregated['ì´ ì „í™˜ë§¤ì¶œì•¡'] / non_search_aggregated['ê´‘ê³ ë¹„']) * 100
                    non_search_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = f"{roas_value:.2f}%"
                else:
                    non_search_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = "0.00%"

                aggregated_rows.append(non_search_aggregated)
                logger.info(f'ë¹„ê²€ìƒ‰ì˜ì—­ í†µí•© ì™„ë£Œ: 1ê°œ í–‰ìœ¼ë¡œ í†µí•©ë¨')
            else:
                logger.info('ë¹„ê²€ìƒ‰ì˜ì—­ ë°ì´í„° ì—†ìŒ')

            # === ë¦¬íƒ€ê²ŸíŒ… í†µí•© ===
            if retargeting_mask.sum() > 0:
                retargeting_df = df[retargeting_mask].copy()
                logger.info(f'ë¦¬íƒ€ê²ŸíŒ… í†µí•© ì „: {retargeting_mask.sum()}ê°œ í–‰')

                # ë¦¬íƒ€ê²ŸíŒ… ì§€í‘œ í•©ì‚°
                retargeting_aggregated = {
                    'í‚¤ì›Œë“œ': 'ë¦¬íƒ€ê²ŸíŒ… (í†µí•©)',
                    'ê´‘ê³  ë…¸ì¶œ ì§€ë©´': 'ë¦¬íƒ€ê²ŸíŒ… (í†µí•©)',
                    'ë…¸ì¶œìˆ˜': retargeting_df['ë…¸ì¶œìˆ˜'].sum(),
                    'í´ë¦­ìˆ˜': retargeting_df['í´ë¦­ìˆ˜'].sum(),
                    'ê´‘ê³ ë¹„': retargeting_df['ê´‘ê³ ë¹„'].sum(),
                    'ì´ ì£¼ë¬¸ìˆ˜': retargeting_df['ì´ ì£¼ë¬¸ìˆ˜'].sum(),
                    'ì´ íŒë§¤ìˆ˜ëŸ‰': retargeting_df['ì´ íŒë§¤ìˆ˜ëŸ‰'].sum(),
                    'ì´ ì „í™˜ë§¤ì¶œì•¡': retargeting_df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()
                }

                # í´ë¦­ë¥  ì¬ê³„ì‚°
                if retargeting_aggregated['ë…¸ì¶œìˆ˜'] > 0:
                    retargeting_aggregated['í´ë¦­ë¥ '] = (retargeting_aggregated['í´ë¦­ìˆ˜'] / retargeting_aggregated['ë…¸ì¶œìˆ˜']) * 100
                else:
                    retargeting_aggregated['í´ë¦­ë¥ '] = 0

                # ROAS ì¬ê³„ì‚° (ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ Excel ë°ì´í„°ì™€ ì¼ì¹˜)
                if retargeting_aggregated['ê´‘ê³ ë¹„'] > 0:
                    roas_value = (retargeting_aggregated['ì´ ì „í™˜ë§¤ì¶œì•¡'] / retargeting_aggregated['ê´‘ê³ ë¹„']) * 100
                    retargeting_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = f"{roas_value:.2f}%"
                else:
                    retargeting_aggregated['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = "0.00%"

                aggregated_rows.append(retargeting_aggregated)
                logger.info(f'ë¦¬íƒ€ê²ŸíŒ… í†µí•© ì™„ë£Œ: 1ê°œ í–‰ìœ¼ë¡œ í†µí•©ë¨')
            else:
                logger.info('ë¦¬íƒ€ê²ŸíŒ… ë°ì´í„° ì—†ìŒ')

            # í†µí•©ëœ ë°ì´í„° ë³‘í•©
            if aggregated_rows:
                aggregated_df = pd.DataFrame(aggregated_rows)
                df = pd.concat([search_only_df, aggregated_df], ignore_index=True)
            else:
                df = search_only_df

        logger.info(f'Total keywords to analyze (before dedup): {len(df)}ê°œ')

        # ğŸ”¥ í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° - ë™ì¼ í‚¤ì›Œë“œëŠ” ë°ì´í„° í•©ì‚°
        if 'í‚¤ì›Œë“œ' in df.columns:
            keyword_groups = df.groupby('í‚¤ì›Œë“œ', as_index=False).agg({
                'ë…¸ì¶œìˆ˜': 'sum',
                'í´ë¦­ìˆ˜': 'sum',
                'ê´‘ê³ ë¹„': 'sum',
                'ì´ ì£¼ë¬¸ìˆ˜': 'sum',
                'ì´ íŒë§¤ìˆ˜ëŸ‰': 'sum',
                'ì´ ì „í™˜ë§¤ì¶œì•¡': 'sum',
                'ê´‘ê³  ë…¸ì¶œ ì§€ë©´': 'first',  # ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            })

            # í´ë¦­ë¥  ì¬ê³„ì‚° (Infinity ë°©ì§€: ë…¸ì¶œìˆ˜ê°€ 0ì¸ ê²½ìš°)
            keyword_groups['í´ë¦­ë¥ '] = (keyword_groups['í´ë¦­ìˆ˜'] / keyword_groups['ë…¸ì¶œìˆ˜'] * 100).replace([np.inf, -np.inf], 0).fillna(0)

            # ROAS ì¬ê³„ì‚°
            keyword_groups['ì´ê´‘ê³ ìˆ˜ìµë¥ '] = keyword_groups.apply(
                lambda row: f"{(row['ì´ ì „í™˜ë§¤ì¶œì•¡'] / row['ê´‘ê³ ë¹„'] * 100):.2f}%" if row['ê´‘ê³ ë¹„'] > 0 else "0.00%",
                axis=1
            )

            df = keyword_groups
            logger.info(f'Keyword deduplication completed: {len(df)}ê°œ (unique keywords)')

        # 2. í´ë¦­ë¥  ì²˜ë¦¬ (ì´ë¯¸ % í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ, ì†Œìˆ˜ì ì´ë©´ 100 ê³±í•˜ê¸°)
        if df['í´ë¦­ë¥ '].max() <= 1:
            df['í´ë¦­ë¥ '] = df['í´ë¦­ë¥ '] * 100

        # 3. ROAS íŒŒì‹±
        if df['ì´ê´‘ê³ ìˆ˜ìµë¥ '].dtype == 'object':
            # "356.78%" â†’ 356.78 ë³€í™˜
            df['ROAS'] = df['ì´ê´‘ê³ ìˆ˜ìµë¥ '].str.rstrip('%').astype(float)
        else:
            df['ROAS'] = df['ì´ê´‘ê³ ìˆ˜ìµë¥ ']
            if df['ROAS'].max() <= 10:  # ì†Œìˆ˜ì  í˜•ì‹ (3.56 â†’ 356)
                df['ROAS'] = df['ROAS'] * 100

        # 4. CPC ê³„ì‚° (í´ë¦­ë‹¹ ë‹¨ê°€) - Infinity ë°©ì§€: í´ë¦­ìˆ˜ê°€ 0ì¸ ê²½ìš°
        df['CPC'] = (df['ê´‘ê³ ë¹„'] / df['í´ë¦­ìˆ˜']).replace([np.inf, -np.inf], 0).fillna(0)

        # 5. ê²°ì¸¡ì¹˜ ë° Infinity ì²˜ë¦¬ (JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)
        # ëª¨ë“  ìˆ«ì ì»¬ëŸ¼ì—ì„œ Infinityì™€ NaNì„ 0ìœ¼ë¡œ ë³€í™˜
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            df[col] = df[col].replace([np.inf, -np.inf], 0).fillna(0)

        logger.info(f'Processed {len(df)} valid keywords')

        # ìš”ì•½ ì§€í‘œ ê³„ì‚°
        total_spend = df['ê´‘ê³ ë¹„'].sum()
        total_revenue = df['ì´ ì „í™˜ë§¤ì¶œì•¡'].sum()

        # í‰ê· CTR ê³„ì‚° (Infinity ë°©ì§€)
        avg_ctr = df['í´ë¦­ë¥ '].mean()
        if np.isinf(avg_ctr) or np.isnan(avg_ctr):
            avg_ctr = 0

        summary = {
            'ì´ê´‘ê³ ë¹„': int(total_spend),
            'ì´ë§¤ì¶œì•¡': int(total_revenue),
            'í‰ê· ROAS': round((total_revenue / total_spend * 100), 2) if total_spend > 0 else 0,
            'ì´í´ë¦­ìˆ˜': int(df['í´ë¦­ìˆ˜'].sum()),
            'í‰ê· CTR': round(float(avg_ctr), 2),
            'ì´ë…¸ì¶œìˆ˜': int(df['ë…¸ì¶œìˆ˜'].sum()),
            'ì´ì£¼ë¬¸ìˆ˜': int(df['ì´ ì£¼ë¬¸ìˆ˜'].sum())
        }

        # JSON ì•ˆì „ ë³€í™˜ í•¨ìˆ˜
        def sanitize_for_json(obj):
            """Infinity, -Infinity, NaNì„ JSON ì•ˆì „ ê°’ìœ¼ë¡œ ë³€í™˜"""
            import math
            # numpy float íƒ€ì…ë„ ì²˜ë¦¬
            if isinstance(obj, (float, np.floating)):
                if math.isinf(float(obj)) or math.isnan(float(obj)):
                    return 0
                return float(obj)  # numpy íƒ€ì…ì„ Python floatìœ¼ë¡œ ë³€í™˜
            elif isinstance(obj, np.integer):
                return int(obj)  # numpy ì •ìˆ˜ë¥¼ Python intë¡œ ë³€í™˜
            return obj

        # DataFrameì„ dictë¡œ ë³€í™˜ í›„ Infinity/NaN ì²˜ë¦¬
        data = df.to_dict('records')
        for row in data:
            for key, value in row.items():
                row[key] = sanitize_for_json(value)

        # ì„¸ì…˜ì— ì €ì¥ (ì„ íƒì‚¬í•­)
        snapshot_id = int(pd.Timestamp.now().timestamp())
        session[f'coupang_snapshot_{snapshot_id}'] = {
            'data': data,
            'summary': summary,
            'created_at': pd.Timestamp.now().isoformat()
        }

        logger.info(f'Coupang data processed successfully: {len(data)} keywords')

        return jsonify({
            'success': True,
            'data': data,
            'summary': summary
        })

    except Exception as e:
        logger.error(f'Coupang file upload failed: {e}')
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'}), 500
